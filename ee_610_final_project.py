# -*- coding: utf-8 -*-
"""EE_610_Final_Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1FG9g_bEtAHELm8w09LWJWQTk3uuEztNl
"""

import os

from google.colab import drive
drive.mount('/content/gdrive')

original_path = '/content/gdrive/MyDrive/Colab Notebooks/EE610_FINAL_PROJECT/DIV2KDATASET_HR/Test'
imageNames_original = []

os.scandir(original_path)

for i in os.scandir(original_path):
  if(str(i.path).find("Thumbs") ==-1): # To eliminate Thumbs.db from list
      imageNames_original.append(i.path)
original_path = '/content/gdrive/MyDrive/Colab Notebooks/EE610_FINAL_PROJECT/DIV2KDATASET_HR/Train'     
for i in os.scandir(original_path):
  if(str(i.path).find("Thumbs") ==-1): # To eliminate Thumbs.db from list
      imageNames_original.append(i.path)       

original_path = '/content/gdrive/MyDrive/Colab Notebooks/EE610_FINAL_PROJECT/DIV2KDATASET_HR/Validation'     
for i in os.scandir(original_path):
  if(str(i.path).find("Thumbs") ==-1): # To eliminate Thumbs.db from list
      imageNames_original.append(i.path)       
        

original_set_size =len(imageNames_original)
print("The no of images in the Train directory is : "+str(original_set_size))



# Plotting sample plots
import matplotlib.pyplot as plt
from matplotlib import image

imageNum = 11 # Eleventh image in the directory will be read

img = image.imread(imageNames_train[imageNum]) # Read image
plt.rcParams["figure.figsize"] = (5,5)
# Print image attributes
print(img.dtype)
print(img.shape)

# Display image
plt.imshow(img)
plt.show()

imageNum =55 # Eleventh image in the directory will be read
img = image.imread(imageNames_train[imageNum]) # Read image

# Print image attributes
print(img.dtype)
print(img.shape)

# Display image
plt.imshow(img)
plt.show()

import cv2
def downSampleImages(imagepath,savepath):
  img = cv2.imread(imagepath, cv2.IMREAD_UNCHANGED)

  temp =imagepath.split("/")
  file_output_path=savepath+"/"+temp[len(temp)-1].split(".")[0]+"_ds_2_1.png"
 

  scale_percent = 50 # percent of original size
  width = int(img.shape[1] * scale_percent / 100)
  height = int(img.shape[0] * scale_percent / 100)
  dimension = (width, height)
  
# resize image
  resized = cv2.resize(img, dimension, interpolation = cv2.INTER_AREA)
 
  cv2.imwrite(file_output_path, resized)
for i in range(len(imageNames_original)):
  downSampleImages(imageNames_original[i],'/content/gdrive/MyDrive/Colab Notebooks/EE610_FINAL_PROJECT/DIV2KDATASET_DS_2_1')

import cv2
def downSampleImages(imagepath,savepath):
  img = cv2.imread(imagepath, cv2.IMREAD_UNCHANGED)

  temp =imagepath.split("/")
  file_output_path=savepath+"/"+temp[len(temp)-1].split(".")[0]+"_ds_4_1.png"
 

  scale_percent = 25 # percent of original size
  width = int(img.shape[1] * scale_percent / 100)
  height = int(img.shape[0] * scale_percent / 100)
  dimension = (width, height)
  
# resize image
  resized = cv2.resize(img, dimension, interpolation = cv2.INTER_AREA)
 
  cv2.imwrite(file_output_path, resized)
for i in range(len(imageNames_original)):
  downSampleImages(imageNames_original[i],'/content/gdrive/MyDrive/Colab Notebooks/EE610_FINAL_PROJECT/DIV2KDATASET_DS_4_1')

import cv2
def upsampleImages_bilinear(imagepath,savepath):
  img = cv2.imread(imagepath, cv2.IMREAD_UNCHANGED)

  temp =imagepath.split("/")
  file_output_path=savepath+"/"+temp[len(temp)-1].split(".")[0].split("_ds_4_1")[0]+"_us_1_4_bilinear.png"
 
  scale_percent = 400 # percent of original size
  width = int(img.shape[1] * scale_percent / 100)
  height = int(img.shape[0] * scale_percent / 100)
  dimension = (width, height)
  
# resize image
  resized = cv2.resize(img, dimension, interpolation = cv2.INTER_LINEAR)
 
  cv2.imwrite(file_output_path, resized)


original_path = '/content/gdrive/MyDrive/Colab Notebooks/EE610_FINAL_PROJECT/DIV2KDATASET_DS_4_1'
imageNames_original = []
os.scandir(original_path)

for i in os.scandir(original_path):
  if(str(i.path).find("Thumbs") ==-1): # To eliminate Thumbs.db from list
      imageNames_original.append(i.path)

for i in range(len(imageNames_original)):
  upsampleImages_bilinear(imageNames_original[i],'/content/gdrive/MyDrive/Colab Notebooks/EE610_FINAL_PROJECT/DIV2KDATASET_US_1_4_BILINEAR')

import cv2
def upsampleImages_bicubic(imagepath,savepath):
  img = cv2.imread(imagepath, cv2.IMREAD_UNCHANGED)

  temp =imagepath.split("/")
  file_output_path=savepath+"/"+temp[len(temp)-1].split(".")[0].split("_ds_4_1")[0]+"_us_1_4_bicubic.png"
 
  scale_percent = 400 # percent of original size
  width = int(img.shape[1] * scale_percent / 100)
  height = int(img.shape[0] * scale_percent / 100)
  dimension = (width, height)
  
# resize image
  resized = cv2.resize(img, dimension, interpolation = cv2.INTER_CUBIC)
  cv2.imwrite(file_output_path, resized)


original_path = '/content/gdrive/MyDrive/Colab Notebooks/EE610_FINAL_PROJECT/DIV2KDATASET_DS_4_1'
imageNames_original = []
os.scandir(original_path)

for i in os.scandir(original_path):
  if(str(i.path).find("Thumbs") ==-1): # To eliminate Thumbs.db from list
      imageNames_original.append(i.path)

for i in range(len(imageNames_original)):
  upsampleImages_bicubic(imageNames_original[i],'/content/gdrive/MyDrive/Colab Notebooks/EE610_FINAL_PROJECT/DIV2KDATASET_US_1_4_BICUBIC')

import cv2
def upsampleImages_lanczos(imagepath,savepath):
  img = cv2.imread(imagepath, cv2.IMREAD_UNCHANGED)

  temp =imagepath.split("/")
  file_output_path=savepath+"/"+temp[len(temp)-1].split(".")[0].split("_ds_4_1")[0]+"_us_1_4_lanczos.png"
 
  scale_percent = 400 # percent of original size
  width = int(img.shape[1] * scale_percent / 100)
  height = int(img.shape[0] * scale_percent / 100)
  dimension = (width, height)
  
# resize image
  resized = cv2.resize(img, dimension, interpolation = cv2.INTER_LANCZOS4)
  cv2.imwrite(file_output_path, resized)


original_path = '/content/gdrive/MyDrive/Colab Notebooks/EE610_FINAL_PROJECT/DIV2KDATASET_DS_4_1/Test'
imageNames_original = []
os.scandir(original_path)

for i in os.scandir(original_path):
  if(str(i.path).find("Thumbs") ==-1): # To eliminate Thumbs.db from list
      imageNames_original.append(i.path)

for i in range(len(imageNames_original)):
  upsampleImages_lanczos(imageNames_original[i],'/content/gdrive/MyDrive/Colab Notebooks/EE610_FINAL_PROJECT/DIV2KDATASET_US_1_4_LANCZOS/Test')

original_path = '/content/gdrive/MyDrive/Colab Notebooks/EE610_FINAL_PROJECT/DIV2KDATASET_DS_4_1/Train'
imageNames_original = []
os.scandir(original_path)

for i in os.scandir(original_path):
  if(str(i.path).find("Thumbs") ==-1): # To eliminate Thumbs.db from list
      imageNames_original.append(i.path)

for i in range(len(imageNames_original)):
  upsampleImages_lanczos(imageNames_original[i],'/content/gdrive/MyDrive/Colab Notebooks/EE610_FINAL_PROJECT/DIV2KDATASET_US_1_4_LANCZOS/Train')

original_path = '/content/gdrive/MyDrive/Colab Notebooks/EE610_FINAL_PROJECT/DIV2KDATASET_DS_4_1/Validation'
imageNames_original = []
os.scandir(original_path)

for i in os.scandir(original_path):
  if(str(i.path).find("Thumbs") ==-1): # To eliminate Thumbs.db from list
      imageNames_original.append(i.path)

for i in range(len(imageNames_original)):
  upsampleImages_lanczos(imageNames_original[i],'/content/gdrive/MyDrive/Colab Notebooks/EE610_FINAL_PROJECT/DIV2KDATASET_US_1_4_LANCZOS/Validation')

import cv2
def downSampleImages_16_1(imagepath,savepath):
  img = cv2.imread(imagepath, cv2.IMREAD_UNCHANGED)

  temp =imagepath.split("/")
  file_output_path=savepath+"/"+temp[len(temp)-1].split(".")[0]+"_ds_16_1.png"
 

  scale_percent = 6.25 # percent of original size
  width = int(img.shape[1] * scale_percent / 100)
  height = int(img.shape[0] * scale_percent / 100)
  dimension = (width, height)
  
# resize image
  resized = cv2.resize(img, dimension, interpolation = cv2.INTER_AREA)
 
  cv2.imwrite(file_output_path, resized)
for i in range(len(imageNames_original)):
  downSampleImages_16_1(imageNames_original[i],'/content/gdrive/MyDrive/Colab Notebooks/EE610_FINAL_PROJECT/DIV2KDATASET_DS_16_1')

import cv2
def upsampleImages_bilinear_1_16(imagepath,savepath):
  img = cv2.imread(imagepath, cv2.IMREAD_UNCHANGED)

  temp =imagepath.split("/")
  file_output_path=savepath+"/"+temp[len(temp)-1].split(".")[0].split("_ds_16_1")[0]+"_us_1_16_bilinear.png"
 
  scale_percent = 1600 # percent of original size
  width = int(img.shape[1] * scale_percent / 100)
  height = int(img.shape[0] * scale_percent / 100)
  dimension = (width, height)
  
# resize image
  resized = cv2.resize(img, dimension, interpolation = cv2.INTER_LINEAR)
  cv2.imwrite(file_output_path, resized)


original_path = '/content/gdrive/MyDrive/Colab Notebooks/EE610_FINAL_PROJECT/DIV2KDATASET_DS_16_1/Test'
imageNames_original = []
os.scandir(original_path)

for i in os.scandir(original_path):
  if(str(i.path).find("Thumbs") ==-1): # To eliminate Thumbs.db from list
      imageNames_original.append(i.path)

for i in range(len(imageNames_original)):
  upsampleImages_bilinear_1_16(imageNames_original[i],'/content/gdrive/MyDrive/Colab Notebooks/EE610_FINAL_PROJECT/DIV2KDATASET_US_1_16_BILINEAR/Test')

original_path = '/content/gdrive/MyDrive/Colab Notebooks/EE610_FINAL_PROJECT/DIV2KDATASET_DS_16_1/Train'
imageNames_original = []
os.scandir(original_path)

for i in os.scandir(original_path):
  if(str(i.path).find("Thumbs") ==-1): # To eliminate Thumbs.db from list
      imageNames_original.append(i.path)

for i in range(len(imageNames_original)):
  upsampleImages_bilinear_1_16(imageNames_original[i],'/content/gdrive/MyDrive/Colab Notebooks/EE610_FINAL_PROJECT/DIV2KDATASET_US_1_16_BILINEAR/Train')

original_path = '/content/gdrive/MyDrive/Colab Notebooks/EE610_FINAL_PROJECT/DIV2KDATASET_DS_16_1/Validation'
imageNames_original = []
os.scandir(original_path)

for i in os.scandir(original_path):
  if(str(i.path).find("Thumbs") ==-1): # To eliminate Thumbs.db from list
      imageNames_original.append(i.path)

for i in range(len(imageNames_original)):
  upsampleImages_bilinear_1_16(imageNames_original[i],'/content/gdrive/MyDrive/Colab Notebooks/EE610_FINAL_PROJECT/DIV2KDATASET_US_1_16_BILINEAR/Validation')

import cv2
def upsampleImages_bicubic_1_16(imagepath,savepath):
  img = cv2.imread(imagepath, cv2.IMREAD_UNCHANGED)

  temp =imagepath.split("/")
  file_output_path=savepath+"/"+temp[len(temp)-1].split(".")[0].split("_ds_16_1")[0]+"_us_1_16_bicubic.png"
 
  scale_percent = 1600 # percent of original size
  width = int(img.shape[1] * scale_percent / 100)
  height = int(img.shape[0] * scale_percent / 100)
  dimension = (width, height)
  
# resize image
  resized = cv2.resize(img, dimension, interpolation = cv2.INTER_CUBIC)
  cv2.imwrite(file_output_path, resized)


original_path = '/content/gdrive/MyDrive/Colab Notebooks/EE610_FINAL_PROJECT/DIV2KDATASET_DS_16_1/Test'
imageNames_original = []
os.scandir(original_path)

for i in os.scandir(original_path):
  if(str(i.path).find("Thumbs") ==-1): # To eliminate Thumbs.db from list
      imageNames_original.append(i.path)

for i in range(len(imageNames_original)):
  upsampleImages_bicubic_1_16(imageNames_original[i],'/content/gdrive/MyDrive/Colab Notebooks/EE610_FINAL_PROJECT/DIV2KDATASET_US_1_16_BICUBIC/Test')

original_path = '/content/gdrive/MyDrive/Colab Notebooks/EE610_FINAL_PROJECT/DIV2KDATASET_DS_16_1/Train'
imageNames_original = []
os.scandir(original_path)

for i in os.scandir(original_path):
  if(str(i.path).find("Thumbs") ==-1): # To eliminate Thumbs.db from list
      imageNames_original.append(i.path)

for i in range(len(imageNames_original)):
  upsampleImages_bicubic_1_16(imageNames_original[i],'/content/gdrive/MyDrive/Colab Notebooks/EE610_FINAL_PROJECT/DIV2KDATASET_US_1_16_BICUBIC/Train')

original_path = '/content/gdrive/MyDrive/Colab Notebooks/EE610_FINAL_PROJECT/DIV2KDATASET_DS_16_1/Validation'
imageNames_original = []
os.scandir(original_path)

for i in os.scandir(original_path):
  if(str(i.path).find("Thumbs") ==-1): # To eliminate Thumbs.db from list
      imageNames_original.append(i.path)

for i in range(len(imageNames_original)):
  upsampleImages_bicubic_1_16(imageNames_original[i],'/content/gdrive/MyDrive/Colab Notebooks/EE610_FINAL_PROJECT/DIV2KDATASET_US_1_16_BICUBIC/Validation')

import cv2
def upsampleImages_lanczos_1_16(imagepath,savepath):
  img = cv2.imread(imagepath, cv2.IMREAD_UNCHANGED)

  temp =imagepath.split("/")
  file_output_path=savepath+"/"+temp[len(temp)-1].split(".")[0].split("_ds_16_1")[0]+"_us_1_16_lanczos.png"
 
  scale_percent = 1600 # percent of original size
  width = int(img.shape[1] * scale_percent / 100)
  height = int(img.shape[0] * scale_percent / 100)
  dimension = (width, height)
  
# resize image
  resized = cv2.resize(img, dimension, interpolation = cv2.INTER_LANCZOS4)
  cv2.imwrite(file_output_path, resized)


original_path = '/content/gdrive/MyDrive/Colab Notebooks/EE610_FINAL_PROJECT/DIV2KDATASET_DS_16_1/Test'
imageNames_original = []
os.scandir(original_path)

for i in os.scandir(original_path):
  if(str(i.path).find("Thumbs") ==-1): # To eliminate Thumbs.db from list
      imageNames_original.append(i.path)

for i in range(len(imageNames_original)):
  upsampleImages_lanczos_1_16(imageNames_original[i],'/content/gdrive/MyDrive/Colab Notebooks/EE610_FINAL_PROJECT/DIV2KDATASET_US_1_16_LANCZOS/Test')

original_path = '/content/gdrive/MyDrive/Colab Notebooks/EE610_FINAL_PROJECT/DIV2KDATASET_DS_16_1/Train'
imageNames_original = []
os.scandir(original_path)

for i in os.scandir(original_path):
  if(str(i.path).find("Thumbs") ==-1): # To eliminate Thumbs.db from list
      imageNames_original.append(i.path)

for i in range(len(imageNames_original)):
  upsampleImages_lanczos_1_16(imageNames_original[i],'/content/gdrive/MyDrive/Colab Notebooks/EE610_FINAL_PROJECT/DIV2KDATASET_US_1_16_LANCZOS/Train')

original_path = '/content/gdrive/MyDrive/Colab Notebooks/EE610_FINAL_PROJECT/DIV2KDATASET_DS_16_1/Validation'
imageNames_original = []
os.scandir(original_path)

for i in os.scandir(original_path):
  if(str(i.path).find("Thumbs") ==-1): # To eliminate Thumbs.db from list
      imageNames_original.append(i.path)

for i in range(len(imageNames_original)):
  upsampleImages_lanczos_1_16(imageNames_original[i],'/content/gdrive/MyDrive/Colab Notebooks/EE610_FINAL_PROJECT/DIV2KDATASET_US_1_16_LANCZOS/Validation')

def createPatches(filepath,filepath_original,test_or_validation,patch_size): 
  global noisyPatches_train
  global centralPixel_train
  global noisyPatches_val
  global centralPixel_val
  global noisyPatches_test
  global centralPixel_test
  img = cv2.imread(filepath)  
  img_original = cv2.imread(filepath_original)  
  if((img.shape[0]<100) or (img.shape[1]<100)):
      return   

  numPatches =patch_size # Extracting patches from image and normalizing
  for i in range(numPatches): # For number of patches
      x = random.randint(0, (img.shape[0]//W)-1) # Random x location so that the patch,remains within image boundaries
      y = random.randint(0, (img.shape[1]//W)-1) # Random y location,remains within image boundaries
      temp_data = np.copy(img[(x*W):((x*W)+W),(y*W):((y*W)+W),:])
      temp_data_central = np.copy(img_original[(x*W):((x*W)+W),(y*W):((y*W)+W),:])
      temp_data = temp_data/255.0 # Extract patch# Divide by 255 for float images
      temp_data_central = temp_data_central/255.0
      temp_central_pixel =np.copy(temp_data_central[(W//2),(W//2),])
      temp_data = temp_data.reshape((W*W*3),1)
    
      temp_central_pixel = temp_central_pixel.reshape((3,1))  
      if(test_or_validation =="TRAIN"): #Appending the noise patches to numpy array for later usage
          noisyPatches_train= np.append(noisyPatches_train,temp_data,axis=1)
          centralPixel_train=np.append(centralPixel_train,temp_central_pixel,axis=1)
      elif(test_or_validation =="VALIDATE"):
          noisyPatches_val= np.append(noisyPatches_val,temp_data,axis=1)
          centralPixel_val=np.append(centralPixel_val,temp_central_pixel,axis=1)
      elif(test_or_validation =="TEST"):
          noisyPatches_test= np.append(noisyPatches_test,temp_data,axis=1)
          centralPixel_test=np.append(centralPixel_test,temp_central_pixel,axis=1)

train_path = '/content/gdrive/MyDrive/Colab Notebooks/EE610_FINAL_PROJECT/DIV2KDATASET_US_1_4_BILINEAR/Train'
val_path = '/content/gdrive/MyDrive/Colab Notebooks/EE610_FINAL_PROJECT/DIV2KDATASET_US_1_4_BILINEAR/Validation'
test_path = '/content/gdrive/MyDrive/Colab Notebooks/EE610_FINAL_PROJECT/DIV2KDATASET_US_1_4_BILINEAR/Test'
original_train_path = '/content/gdrive/MyDrive/Colab Notebooks/EE610_FINAL_PROJECT/DIV2KDATASET_HR/Train'
original_test_path = '/content/gdrive/MyDrive/Colab Notebooks/EE610_FINAL_PROJECT/DIV2KDATASET_HR/Test'
original_val_path = '/content/gdrive/MyDrive/Colab Notebooks/EE610_FINAL_PROJECT/DIV2KDATASET_HR/Validation'

imageNames_train = []
imageNames_val = []
imageNames_test = []
original_imageNames_train =[]
original_imageNames_test =[]
original_imageNames_val =[]

os.scandir(train_path)

for i in os.scandir(train_path):
  if(str(i.path).find("Thumbs") ==-1): # To eliminate Thumbs.db from list
      imageNames_train.append(i.path)
for i in os.scandir(val_path):
  if(str(i.path).find("Thumbs") ==-1): # To eliminate Thumbs.db from list
      imageNames_val.append(i.path)
for i in os.scandir(test_path):
  if(str(i.path).find("Thumbs") ==-1): # To eliminate Thumbs.db from list
      imageNames_test.append(i.path)
for i in os.scandir(original_train_path):
  if(str(i.path).find("Thumbs") ==-1): # To eliminate Thumbs.db from list
      original_imageNames_train.append(i.path)
for i in os.scandir(original_test_path):
  if(str(i.path).find("Thumbs") ==-1): # To eliminate Thumbs.db from list
      original_imageNames_test.append(i.path)
for i in os.scandir(original_val_path):
  if(str(i.path).find("Thumbs") ==-1): # To eliminate Thumbs.db from list
      original_imageNames_val.append(i.path)
        
imageNames_train =sorted(imageNames_train)     
original_imageNames_train =sorted(original_imageNames_train)  
imageNames_test =sorted(imageNames_test)     
original_imageNames_test =sorted(original_imageNames_test) 
imageNames_val =sorted(imageNames_val)     
original_imageNames_val =sorted(original_imageNames_val)   
        
training_set_size =len(imageNames_train)
print("The no of images in the Train directory is : "+str(training_set_size))

val_set_size =len(imageNames_val)
print("The no of images in the Validation directory is : "+str(val_set_size))

test_set_size =len(imageNames_test)
print("The no of images in the Test directory is : "+str(test_set_size))

from matplotlib import pyplot as plt
import cv2
import numpy as np
import random
global noisyPatches_train
global centralPixel_train
global noisyPatches_val
global centralPixel_val
global noisyPatches_test
global centralPixel_test

global W
W=27 

noisyPatches_train = np.zeros((W*W*3,0),dtype=np.float64)
centralPixel_train = np.zeros((3,0),dtype=np.float64)
noisyPatches_val = np.zeros((W*W*3,0),dtype=np.float64)
centralPixel_val = np.zeros((3,0),dtype=np.float64)
noisyPatches_test = np.zeros((W*W*3,0),dtype=np.float64)
centralPixel_test = np.zeros((3,0),dtype=np.float64)

for i in range(len(imageNames_train)):
    createPatches(imageNames_train[i],original_imageNames_train[i],"TRAIN",100)
for i in range(len(imageNames_val)):
    createPatches(imageNames_val[i],original_imageNames_val[i],"VALIDATE",100)
for i in range(len(imageNames_test)):
    createPatches(imageNames_test[i],original_imageNames_test[i],"TEST",100)    


    
np.save('/content/gdrive/MyDrive/Colab Notebooks/EE610_FINAL_PROJECT/Outputs/noisyPatches_train_bilinear_1_4', noisyPatches_train)
np.save('/content/gdrive/MyDrive/Colab Notebooks/EE610_FINAL_PROJECT/Outputs/centralPixel_train_bilinear_1_4', centralPixel_train)
np.save('/content/gdrive/MyDrive/Colab Notebooks/EE610_FINAL_PROJECT/Outputs/noisyPatches_val_bilinear_1_4', noisyPatches_val)
np.save('/content/gdrive/MyDrive/Colab Notebooks/EE610_FINAL_PROJECT/Outputs/centralPixel_val_bilinear_1_4', centralPixel_val)
np.save('/content/gdrive/MyDrive/Colab Notebooks/EE610_FINAL_PROJECT/Outputs/noisyPatches_test_bilinear_1_4', noisyPatches_test)
np.save('/content/gdrive/MyDrive/Colab Notebooks/EE610_FINAL_PROJECT/Outputs/centralPixel_test_bilinear_1_4', centralPixel_test)

print("Noise Patch (Train) shape : ",noisyPatches_train.shape)
print("Central Pixel (Train) shape : ",centralPixel_train.shape)
print("Noise Patch (Validation) shape : ",noisyPatches_val.shape)
print("Central Pixel (Validation)  shape : ",centralPixel_val.shape)
print("Noise Patch(Test) shape : ",noisyPatches_test.shape)
print("Central Pixel (Test) shape : ",centralPixel_test.shape)

"""# subPatchVector Function
This module extact subpatches of (n\*n) size where (n<=13) from the already extracted (13\*13) patch vector.
"""

global W
W=27 
def subPatchVectors(training_vector,sub_patch_size):
    length =training_vector.shape[1]
    start =int((W-sub_patch_size)/2)
    
    temp_var = np.copy(training_vector)
    temp_var_1 = temp_var.reshape(W,W,3,length)
    temp_var_1 =temp_var_1[start:W-start,start:W-start,:,:]
    temp_var_1=temp_var_1.reshape(sub_patch_size*sub_patch_size*3,length)
    
    return temp_var_1

"""## Function to reconstruct image
The function reconstructImage repair the image with the neural network model. The edges (W/2 pixels) are copied directly
from the corrupted image. For all other pixels, a patch is mined from the corrupted image (W*W) with the corresponding image as 
central pixel. The image pixels from the patch is fed to the machine learning model. The central pixel is replaced by the output of machine learning model.
"""

global W
import cv2

global model
from sklearn.datasets import load_sample_image
from sklearn.feature_extraction import image

def reconstructImage_ANN(filepath,model_,subpatch_size):
    img = cv2.imread(filepath)
    rgb_img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)

    img = img.astype(np.float64)
    img = img/255.0

    border = int(subpatch_size/2)
    temp_image = cv2.copyMakeBorder(img, border, border, border, border, cv2.BORDER_REFLECT)
    temp_image_patches=image.extract_patches_2d(temp_image, (subpatch_size,subpatch_size))

    temp_image_patches = temp_image_patches.reshape(temp_image_patches.shape[0],subpatch_size*subpatch_size*3)
    predicted_value =model_.predict(temp_image_patches)
    predicted_value = predicted_value.reshape(img.shape[0],img.shape[1],img.shape[2])
    predicted_value = predicted_value*255.0;
    predicted_value = predicted_value.astype(np.uint8)



    return predicted_value

global W
import cv2

global model
from sklearn.datasets import load_sample_image
from sklearn.feature_extraction import image

def reconstructImage_CNN(filepath,model_,sub_patch):
    img = cv2.imread(filepath)
    #print(img.shape)
    rgb_img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)

    img = img.astype(np.float64)
    img = img/255.0

    border = int(sub_patch/2)
    temp_image = cv2.copyMakeBorder(img, border, border, border, border, cv2.BORDER_REFLECT)
    temp_image_patches=image.extract_patches_2d(temp_image, (sub_patch,sub_patch))
    print(temp_image_patches.shape)
    predicted_value =model_.predict(temp_image_patches)
    print(predicted_value.shape)
    predicted_value = predicted_value.reshape(img.shape[0],img.shape[1],img.shape[2])
    predicted_value = predicted_value*255.0;
    predicted_value = predicted_value.astype(np.uint8)
    return predicted_value

import numpy as np
def calculateNoise(filepath_original,filepath_downsampled,filepath_reconstructed):
    img_original = cv2.imread(filepath_original)
    rgb_original = cv2.cvtColor(img_original, cv2.COLOR_BGR2RGB)

    img_downsampled = cv2.imread(filepath_downsampled)
    rgb_downsampled = cv2.cvtColor(img_downsampled, cv2.COLOR_BGR2RGB)

    img_reconstructed = cv2.imread(filepath_reconstructed)
    rgb_reconstructed = cv2.cvtColor(img_reconstructed, cv2.COLOR_BGR2RGB)

    height = img_original.shape[0]
    width = img_original.shape[1]
    depth = img_original.shape[2]

    noise_downsampled = np.sqrt(np.sum(np.square(np.subtract(rgb_original,rgb_downsampled)))/(height*width*depth))
    noise_reconstructed = np.sqrt(np.sum(np.square(np.subtract(rgb_original,rgb_reconstructed)))/(height*width*depth))

    
    return noise_downsampled,  noise_reconstructed

"""### Model-1 (Layer 11)Definition

The function createModel implements and initialises a neural network regression model. No of layers in neural network is 11. 
The input layer and hidden layers have relu activation function. The output layer has sigmoid activation function as the output 
has to be finally scaled to [0 to 255] range. Sigmoid activation function returns value between 0 to 1 which when multiplied with 255 will revert back to [0 to 255] range. RELU activation function is selected for input and inner layers for fast training. Loss function is taken as 'mse'(mean squared error). Optimization algorithm selected is 'ADAM' for fast training. Stop loss function is implemented to avoid overfitting of data. Stop loss monitoring parameter is validation_loss with minimum delta of 0.0001. Restore best weight feature is enabled in stop loss function to revert the weight to the optimal value in the stop loss window.<br>

Input layer : 507 (5\*5\*3)<br>
Layer 1: 4096<br>
Layer 2: 2048<br>
Layer 3: 1024<br>
Layer 4: 512<br>
Layer 5: 256<br>
Layer 6: 128<br>
Layer 7: 64<br>
Layer 8: 32<br>
Layer 9: 16<br>
Layer 10: 8<br>
Layer 11(Output layer): 3<br>

Subpatch Size : 5
"""

# import numpy as np
from keras.models import Sequential
from keras.layers import Dense
import pandas as pd
from keras.callbacks import EarlyStopping
import numpy as np
from matplotlib import pyplot as plt


def createModel_1(noisyPatches_train1,centralPixel_train1,noisyPatches_val1,centralPixel_val1,noisyPatches_test1,centralPixel_test1,PatchSize,model_name):
    print("Traiing and Validation sets size :",noisyPatches_train1.shape,centralPixel_train1.shape,noisyPatches_val1.shape,centralPixel_val1.shape)
    es = EarlyStopping(monitor='val_loss', mode='min',min_delta=0.0001, verbose=1,patience=15,restore_best_weights=True)
    model = Sequential()
    model.add(Dense(4096, input_dim=PatchSize*PatchSize*3, activation='relu'))
    model.add(Dense(2048, activation='relu'))  
    model.add(Dense(1024, activation='relu'))
    model.add(Dense(512, activation='relu'))
    model.add(Dense(256, activation='relu'))
    model.add(Dense(128, activation='relu'))
    model.add(Dense(64, activation='relu'))
    model.add(Dense(32, activation='relu'))
    model.add(Dense(16, activation='relu'))
    model.add(Dense(8, activation='relu'))
    model.add(Dense(3, activation='sigmoid'))
    model.compile(loss='mse', optimizer='adam', metrics=['accuracy'])
    history = model.fit(noisyPatches_train1.T, centralPixel_train1.T, epochs=1000,callbacks=[es],validation_data=(noisyPatches_val1.T, centralPixel_val1.T))

    pd.DataFrame(history.history).plot(figsize=(8, 5))
    plt.grid(True)
    plt.gca().set_ylim(0, 1) # set the vertical range to [0-1]
    plt.show()

    _, accuracy =model.evaluate(noisyPatches_test1.T, centralPixel_test1.T)
    print('Accuracy: %.2f' % (accuracy*100))
    model.save(model_name) # Saving model information for later usage
    #reconstructed_model = keras.models.load_model("my_model")
    return model

"""### Creating Model 1_5 (ANN Layer 7, Subpatch size:5)"""

import numpy as np
noisyPatches_train_1 = np.load('/content/gdrive/MyDrive/Colab Notebooks/EE610_FINAL_PROJECT/Outputs/noisyPatches_train_bilinear_1_4.npy')
centralPixel_train_1 = np.load('/content/gdrive/MyDrive/Colab Notebooks/EE610_FINAL_PROJECT/Outputs/centralPixel_train_bilinear_1_4.npy')
noisyPatches_val_1 = np.load('/content/gdrive/MyDrive/Colab Notebooks/EE610_FINAL_PROJECT/Outputs/noisyPatches_val_bilinear_1_4.npy')
centralPixel_val_1 = np.load('/content/gdrive/MyDrive/Colab Notebooks/EE610_FINAL_PROJECT/Outputs/centralPixel_val_bilinear_1_4.npy')
noisyPatches_test_1 = np.load('/content/gdrive/MyDrive/Colab Notebooks/EE610_FINAL_PROJECT/Outputs/noisyPatches_test_bilinear_1_4.npy')
centralPixel_test_1 = np.load('/content/gdrive/MyDrive/Colab Notebooks/EE610_FINAL_PROJECT/Outputs/centralPixel_test_bilinear_1_4.npy')

noisyPatches_train_sub = subPatchVectors(noisyPatches_train_1,5)
noisyPatches_val_sub = subPatchVectors(noisyPatches_val_1,5)
noisyPatches_test_sub = subPatchVectors(noisyPatches_test_1,5)

model1_5_1_4_bilinear = createModel_1(noisyPatches_train_sub,centralPixel_train_1,noisyPatches_val_sub,centralPixel_val_1,noisyPatches_test_sub,centralPixel_test_1,5,"/content/gdrive/MyDrive/Colab Notebooks/EE610_FINAL_PROJECT/Outputs/model1_5_1_4_bilinear.h5")

corrrected_image1 =reconstructImage_ANN('/content/gdrive/MyDrive/Colab Notebooks/EE610_FINAL_PROJECT/DIV2KDATASET_US_1_4_BILINEAR/Test/0228_us_1_4_bilinear.png',model1_5_1_4_bilinear,5)
cv2.imwrite('/content/gdrive/MyDrive/Colab Notebooks/EE610_FINAL_PROJECT/Outputs/0228_us_1_4_bilinear_model1_5_1_4_bilinear.png', corrrected_image1)

noise_downsampled,  noise_reconstructed=calculateNoise('/content/gdrive/MyDrive/Colab Notebooks/EE610_FINAL_PROJECT/DIV2KDATASET_HR/Test/0228.png','/content/gdrive/MyDrive/Colab Notebooks/EE610_FINAL_PROJECT/DIV2KDATASET_US_1_4_BILINEAR/Test/0228_us_1_4_bilinear.png','/content/gdrive/MyDrive/Colab Notebooks/EE610_FINAL_PROJECT/Outputs/0228_us_1_4_bilinear_model1_5_1_4_bilinear.png')
print("Noise in Downsampled and Upsampled Image :",noise_downsampled)
print("Noise in reconstructed Image :",noise_reconstructed)

corrrected_image1 =reconstructImage_ANN('/content/gdrive/MyDrive/Colab Notebooks/EE610_FINAL_PROJECT/Inputs/228_low_quality_bilinear.png',model1_5_1_4_bilinear,5)
cv2.imwrite('/content/gdrive/MyDrive/Colab Notebooks/EE610_FINAL_PROJECT/Outputs/228_low_quality_bilinear_model1_5_1_4_bilinear.png', corrrected_image1)

noise_downsampled,  noise_reconstructed=calculateNoise('/content/gdrive/MyDrive/Colab Notebooks/EE610_FINAL_PROJECT/Inputs/228.png','/content/gdrive/MyDrive/Colab Notebooks/EE610_FINAL_PROJECT/Inputs/228_low_quality_bilinear.png','/content/gdrive/MyDrive/Colab Notebooks/EE610_FINAL_PROJECT/Outputs/228_low_quality_bilinear_model1_5_1_4_bilinear.png')
print("Noise in Downsampled and Upsampled Image :",noise_downsampled)
print("Noise in reconstructed Image :",noise_reconstructed)

corrrected_image1 =reconstructImage_ANN('/content/gdrive/MyDrive/Colab Notebooks/EE610_FINAL_PROJECT/Inputs/butterfly_low_quality_bilinear.png',model1_5_1_4_bilinear,5)
cv2.imwrite('/content/gdrive/MyDrive/Colab Notebooks/EE610_FINAL_PROJECT/Outputs/butterfly_low_quality_bilinear_model1_5_1_4_bilinear.png', corrrected_image1)

noise_downsampled,  noise_reconstructed=calculateNoise('/content/gdrive/MyDrive/Colab Notebooks/EE610_FINAL_PROJECT/Inputs/butterfly.jpg','/content/gdrive/MyDrive/Colab Notebooks/EE610_FINAL_PROJECT/Inputs/butterfly_low_quality_bilinear.png','/content/gdrive/MyDrive/Colab Notebooks/EE610_FINAL_PROJECT/Outputs/butterfly_low_quality_bilinear_model1_5_1_4_bilinear.png')
print("Noise in Downsampled and Upsampled Image :",noise_downsampled)
print("Noise in reconstructed Image :",noise_reconstructed)

import cv2
  img = cv2.imread('/content/gdrive/MyDrive/Colab Notebooks/EE610_FINAL_PROJECT/Inputs/butterfly.jpg', cv2.IMREAD_UNCHANGED)
  scale_percent = 25 # percent of original size
  width = int(img.shape[1] * scale_percent / 100)
  height = int(img.shape[0] * scale_percent / 100)
  dimension = (width, height)
  resized = cv2.resize(img, dimension, interpolation = cv2.INTER_LINEAR)
  #cv2.imwrite('/content/gdrive/MyDrive/Colab Notebooks/EE610_FINAL_PROJECT/Inputs/butterfly_ds_4_1.png', resized)
    
  scale_percent = 400 # percent of original size
  width = int(resized.shape[1] * scale_percent / 100)
  height = int(resized.shape[0] * scale_percent / 100)
  dimension = (width, height)
  resized_ = cv2.resize(resized, dimension, interpolation = cv2.INTER_LINEAR)
  cv2.imwrite('/content/gdrive/MyDrive/Colab Notebooks/EE610_FINAL_PROJECT/Inputs/butterfly_low_quality_bilinear.png', resized_)

import cv2
  img = cv2.imread('/content/gdrive/MyDrive/Colab Notebooks/EE610_FINAL_PROJECT/Inputs/228.png', cv2.IMREAD_UNCHANGED)
  scale_percent = 25 # percent of original size
  width = int(img.shape[1] * scale_percent / 100)
  height = int(img.shape[0] * scale_percent / 100)
  dimension = (width, height)
  resized = cv2.resize(img, dimension, interpolation = cv2.INTER_LINEAR)
  #cv2.imwrite('/content/gdrive/MyDrive/Colab Notebooks/EE610_FINAL_PROJECT/Inputs/butterfly_ds_4_1.png', resized)
    
  scale_percent = 400 # percent of original size
  width = int(resized.shape[1] * scale_percent / 100)
  height = int(resized.shape[0] * scale_percent / 100)
  dimension = (width, height)
  resized_ = cv2.resize(resized, dimension, interpolation = cv2.INTER_LINEAR)
  cv2.imwrite('/content/gdrive/MyDrive/Colab Notebooks/EE610_FINAL_PROJECT/Inputs/228_low_quality_bilinear.png', resized_)

"""### Creating Model 1_15 (ANN Layer 7, Subpatch size:15)"""

import numpy as np
noisyPatches_train_1 = np.load('/content/gdrive/MyDrive/Colab Notebooks/EE610_FINAL_PROJECT/Outputs/noisyPatches_train_bilinear_1_4.npy')
centralPixel_train_1 = np.load('/content/gdrive/MyDrive/Colab Notebooks/EE610_FINAL_PROJECT/Outputs/centralPixel_train_bilinear_1_4.npy')
noisyPatches_val_1 = np.load('/content/gdrive/MyDrive/Colab Notebooks/EE610_FINAL_PROJECT/Outputs/noisyPatches_val_bilinear_1_4.npy')
centralPixel_val_1 = np.load('/content/gdrive/MyDrive/Colab Notebooks/EE610_FINAL_PROJECT/Outputs/centralPixel_val_bilinear_1_4.npy')
noisyPatches_test_1 = np.load('/content/gdrive/MyDrive/Colab Notebooks/EE610_FINAL_PROJECT/Outputs/noisyPatches_test_bilinear_1_4.npy')
centralPixel_test_1 = np.load('/content/gdrive/MyDrive/Colab Notebooks/EE610_FINAL_PROJECT/Outputs/centralPixel_test_bilinear_1_4.npy')

noisyPatches_train_sub = subPatchVectors(noisyPatches_train_1,15)
noisyPatches_val_sub = subPatchVectors(noisyPatches_val_1,15)
noisyPatches_test_sub = subPatchVectors(noisyPatches_test_1,15)

model1_15_1_4_bilinear = createModel_1(noisyPatches_train_sub,centralPixel_train_1,noisyPatches_val_sub,centralPixel_val_1,noisyPatches_test_sub,centralPixel_test_1,15,"/content/gdrive/MyDrive/Colab Notebooks/EE610_FINAL_PROJECT/Outputs/model1_15_1_4_bilinear.h5")

corrrected_image1 =reconstructImage_ANN('/content/gdrive/MyDrive/Colab Notebooks/EE610_FINAL_PROJECT/Inputs/butterfly_low_quality_bilinear.png',model1_15_1_4_bilinear,15)
cv2.imwrite('/content/gdrive/MyDrive/Colab Notebooks/EE610_FINAL_PROJECT/Outputs/butterfly_low_quality_bilinear_model1_15_1_4_bilinear.png', corrrected_image1)

noise_downsampled,  noise_reconstructed=calculateNoise('/content/gdrive/MyDrive/Colab Notebooks/EE610_FINAL_PROJECT/Inputs/butterfly.jpg','/content/gdrive/MyDrive/Colab Notebooks/EE610_FINAL_PROJECT/Inputs/butterfly_low_quality_bilinear.png','/content/gdrive/MyDrive/Colab Notebooks/EE610_FINAL_PROJECT/Outputs/butterfly_low_quality_bilinear_model1_15_1_4_bilinear.png')
print("Noise in Downsampled and Upsampled Image :",noise_downsampled)
print("Noise in reconstructed Image :",noise_reconstructed)

# import numpy as np
from keras.models import Sequential
from keras.layers import Dense
import pandas as pd
from keras.callbacks import EarlyStopping
import numpy as np
from matplotlib import pyplot as plt


def createModel_4(noisyPatches_train1,centralPixel_train1,noisyPatches_val1,centralPixel_val1,noisyPatches_test1,centralPixel_test1,PatchSize,model_name):
    print("Traiing and Validation sets size :",noisyPatches_train1.shape,centralPixel_train1.shape,noisyPatches_val1.shape,centralPixel_val1.shape)
    es = EarlyStopping(monitor='val_loss', mode='min',min_delta=0.0001, verbose=1,patience=15,restore_best_weights=True)
    model = Sequential()
    model.add(Dense(1024, input_dim=PatchSize*PatchSize*3, activation='relu'))
    model.add(Dense(512, activation='relu'))
    model.add(Dense(256, activation='relu'))
    model.add(Dense(128, activation='relu'))
    model.add(Dense(64, activation='relu'))
    model.add(Dense(32, activation='relu'))
    model.add(Dense(16, activation='relu'))
    model.add(Dense(8, activation='relu'))
    model.add(Dense(3, activation='sigmoid'))
    model.compile(loss='mse', optimizer='adam', metrics=['accuracy'])
    history = model.fit(noisyPatches_train1.T, centralPixel_train1.T, epochs=1000,callbacks=[es],validation_data=(noisyPatches_val1.T, centralPixel_val1.T))

    pd.DataFrame(history.history).plot(figsize=(8, 5))
    plt.grid(True)
    plt.gca().set_ylim(0, 1) # set the vertical range to [0-1]
    plt.show()

    _, accuracy =model.evaluate(noisyPatches_test1.T, centralPixel_test1.T)
    print('Accuracy: %.2f' % (accuracy*100))
    model.save(model_name) # Saving model information for later usage
    #reconstructed_model = keras.models.load_model("my_model")
    return model

import numpy as np
noisyPatches_train_1 = np.load('/content/gdrive/MyDrive/Colab Notebooks/EE610_FINAL_PROJECT/Outputs/noisyPatches_train_bilinear_1_4.npy')
centralPixel_train_1 = np.load('/content/gdrive/MyDrive/Colab Notebooks/EE610_FINAL_PROJECT/Outputs/centralPixel_train_bilinear_1_4.npy')
noisyPatches_val_1 = np.load('/content/gdrive/MyDrive/Colab Notebooks/EE610_FINAL_PROJECT/Outputs/noisyPatches_val_bilinear_1_4.npy')
centralPixel_val_1 = np.load('/content/gdrive/MyDrive/Colab Notebooks/EE610_FINAL_PROJECT/Outputs/centralPixel_val_bilinear_1_4.npy')
noisyPatches_test_1 = np.load('/content/gdrive/MyDrive/Colab Notebooks/EE610_FINAL_PROJECT/Outputs/noisyPatches_test_bilinear_1_4.npy')
centralPixel_test_1 = np.load('/content/gdrive/MyDrive/Colab Notebooks/EE610_FINAL_PROJECT/Outputs/centralPixel_test_bilinear_1_4.npy')

noisyPatches_train_sub = subPatchVectors(noisyPatches_train_1,5)
noisyPatches_val_sub = subPatchVectors(noisyPatches_val_1,5)
noisyPatches_test_sub = subPatchVectors(noisyPatches_test_1,5)

model4_5_1_4_bilinear = createModel_4(noisyPatches_train_sub,centralPixel_train_1,noisyPatches_val_sub,centralPixel_val_1,noisyPatches_test_sub,centralPixel_test_1,5,"/content/gdrive/MyDrive/Colab Notebooks/EE610_FINAL_PROJECT/Outputs/model4_5_1_4_bilinear.h5")

corrrected_image1 =reconstructImage_ANN('/content/gdrive/MyDrive/Colab Notebooks/EE610_FINAL_PROJECT/DIV2KDATASET_US_1_4_BILINEAR/Test/0228_us_1_4_bilinear.png',model4_5_1_4_bilinear,5)
cv2.imwrite('/content/gdrive/MyDrive/Colab Notebooks/EE610_FINAL_PROJECT/Outputs/0228_us_1_4_bilinear_model4_5_1_4_bilinear.png', corrrected_image1)

noise_downsampled,  noise_reconstructed=calculateNoise('/content/gdrive/MyDrive/Colab Notebooks/EE610_FINAL_PROJECT/DIV2KDATASET_HR/Test/0228.png','/content/gdrive/MyDrive/Colab Notebooks/EE610_FINAL_PROJECT/DIV2KDATASET_US_1_4_BILINEAR/Test/0228_us_1_4_bilinear.png','/content/gdrive/MyDrive/Colab Notebooks/EE610_FINAL_PROJECT/Outputs/0228_us_1_4_bilinear_model4_5_1_4_bilinear.png')
print("Noise in Downsampled and Upsampled Image :",noise_downsampled)
print("Noise in reconstructed Image :",noise_reconstructed)

corrrected_image1 =reconstructImage_ANN('/content/gdrive/MyDrive/Colab Notebooks/EE610_FINAL_PROJECT/DIV2KDATASET_US_1_4_BILINEAR/Test/0267_us_1_4_bilinear.png',model4_5_1_4_bilinear,5)
cv2.imwrite('/content/gdrive/MyDrive/Colab Notebooks/EE610_FINAL_PROJECT/Outputs/0267_us_1_4_bilinear_model4_5_1_4_bilinear.png', corrrected_image1)

noise_downsampled,  noise_reconstructed=calculateNoise('/content/gdrive/MyDrive/Colab Notebooks/EE610_FINAL_PROJECT/DIV2KDATASET_HR/Test/0267.png','/content/gdrive/MyDrive/Colab Notebooks/EE610_FINAL_PROJECT/DIV2KDATASET_US_1_4_BILINEAR/Test/0267_us_1_4_bilinear.png','/content/gdrive/MyDrive/Colab Notebooks/EE610_FINAL_PROJECT/Outputs/0267_us_1_4_bilinear_model4_5_1_4_bilinear.png')
print("Noise in Downsampled and Upsampled Image :",noise_downsampled)
print("Noise in reconstructed Image :",noise_reconstructed)

import numpy as np
noisyPatches_train_1 = np.load('/content/gdrive/MyDrive/Colab Notebooks/EE610_FINAL_PROJECT/Outputs/noisyPatches_train_bilinear_1_4.npy')
centralPixel_train_1 = np.load('/content/gdrive/MyDrive/Colab Notebooks/EE610_FINAL_PROJECT/Outputs/centralPixel_train_bilinear_1_4.npy')
noisyPatches_val_1 = np.load('/content/gdrive/MyDrive/Colab Notebooks/EE610_FINAL_PROJECT/Outputs/noisyPatches_val_bilinear_1_4.npy')
centralPixel_val_1 = np.load('/content/gdrive/MyDrive/Colab Notebooks/EE610_FINAL_PROJECT/Outputs/centralPixel_val_bilinear_1_4.npy')
noisyPatches_test_1 = np.load('/content/gdrive/MyDrive/Colab Notebooks/EE610_FINAL_PROJECT/Outputs/noisyPatches_test_bilinear_1_4.npy')
centralPixel_test_1 = np.load('/content/gdrive/MyDrive/Colab Notebooks/EE610_FINAL_PROJECT/Outputs/centralPixel_test_bilinear_1_4.npy')

noisyPatches_train_sub = subPatchVectors(noisyPatches_train_1,15)
noisyPatches_val_sub = subPatchVectors(noisyPatches_val_1,15)
noisyPatches_test_sub = subPatchVectors(noisyPatches_test_1,15)

model4_15_1_4_bilinear = createModel_4(noisyPatches_train_sub,centralPixel_train_1,noisyPatches_val_sub,centralPixel_val_1,noisyPatches_test_sub,centralPixel_test_1,15,"/content/gdrive/MyDrive/Colab Notebooks/EE610_FINAL_PROJECT/Outputs/model4_15_1_4_bilinear.h5")

import keras as keras
model1_15_1_4_bilinear = keras.models.load_model("/content/gdrive/MyDrive/Colab Notebooks/EE610_FINAL_PROJECT/Outputs/model4_15_1_4_bilinear.h5")
corrrected_image1 =reconstructImage_ANN('/content/gdrive/MyDrive/Colab Notebooks/EE610_FINAL_PROJECT/Inputs/228_low_quality_bilinear.png',model1_15_1_4_bilinear,15)
cv2.imwrite('/content/gdrive/MyDrive/Colab Notebooks/EE610_FINAL_PROJECT/Outputs/228_low_quality_bilinear_model4_15_1_4_bilinear.png', corrrected_image1)

noise_downsampled,  noise_reconstructed=calculateNoise('/content/gdrive/MyDrive/Colab Notebooks/EE610_FINAL_PROJECT/Inputs/228.png','/content/gdrive/MyDrive/Colab Notebooks/EE610_FINAL_PROJECT/Inputs/228_low_quality_bilinear.png','/content/gdrive/MyDrive/Colab Notebooks/EE610_FINAL_PROJECT/Outputs/228_low_quality_bilinear_model1_15_1_4_bilinear.png')
print("Noise in Downsampled and Upsampled Image :",noise_downsampled)
print("Noise in reconstructed Image :",noise_reconstructed)



"""### Model-2 (Layer 2, Patch Size : 7)Definition CNN

### CNN Details
Input : (7\*7\*3) <br>
First layer : 256 filters, (7\*7)<br>
Max pooling layer<br>
Second layer : 256 fliters(5\*5)<br>
Max pooling<br>
Fully connected layer :64<br>
FC layer : 16<br>
FC layer :3<br>
"""

# import numpy as np
import keras.models as models
import keras.layers as layers
from keras.models import Sequential
from keras.layers import Dense
import pandas as pd
from keras.callbacks import EarlyStopping
import numpy as np
from matplotlib import pyplot as plt

def createModel_2(noisyPatches_train1,centralPixel_train1,noisyPatches_val1,centralPixel_val1,noisyPatches_test1,centralPixel_test1,PatchSize,model_name):
    print("Traiing and Validation sets size :",noisyPatches_train1.shape,centralPixel_train1.shape,noisyPatches_val1.shape,centralPixel_val1.shape)
    
    if(PatchSize!=7):
      print("Expected Patch size is 7: Error condition")
      return

    row =noisyPatches_train1.shape[0] #507
    coloumn =noisyPatches_train1.shape[1]# 16250
    noisyPatches_train_1 = np.swapaxes(noisyPatches_train1,0,1)
    noisyPatches_train_1 = noisyPatches_train_1.reshape((coloumn,PatchSize,PatchSize,3))

    row =noisyPatches_val1.shape[0] #507
    coloumn =noisyPatches_val1.shape[1]# 16250
    noisyPatches_val_1 = np.swapaxes(noisyPatches_val1,0,1)
    noisyPatches_val_1 = noisyPatches_val_1.reshape((coloumn,PatchSize,PatchSize,3))

    row =noisyPatches_test1.shape[0] #507
    coloumn =noisyPatches_test1.shape[1]# 16250
    noisyPatches_test_1 = np.swapaxes(noisyPatches_test1,0,1)
    noisyPatches_test_1 = noisyPatches_test_1.reshape((coloumn,PatchSize,PatchSize,3))


    es = EarlyStopping(monitor='val_loss', mode='min',min_delta=0.0001, verbose=1,patience=15,restore_best_weights=True)
    model = models.Sequential()
    model.add(layers.Conv2D(256, (7, 7), activation='relu',padding='same', input_shape=(7, 7, 3)))
    model.add(layers.MaxPooling2D((2, 2)))
    model.add(layers.Conv2D(256, (5, 5), activation='relu',padding='same'))
    model.add(layers.MaxPooling2D((2, 2)))
    model.add(layers.Flatten())
    model.add(layers.Dense(64, activation='relu'))
    model.add(layers.Dense(10))
    model.add(layers.Dense(3, activation='sigmoid'))
    model.summary()
    model.compile(loss='mse', optimizer='adam', metrics=['accuracy'])
    history = model.fit(noisyPatches_train_1, centralPixel_train1.T, epochs=1000,callbacks=[es],validation_data=(noisyPatches_val_1, centralPixel_val1.T))

    pd.DataFrame(history.history).plot(figsize=(8, 5))
    plt.grid(True)
    plt.gca().set_ylim(0, 1) # set the vertical range to [0-1]
    plt.show()

    _, accuracy =model.evaluate(noisyPatches_test_1, centralPixel_test1.T)
    print('Accuracy: %.2f' % (accuracy*100))
    model.save(model_name) # Saving model information for later usage
    #reconstructed_model = keras.models.load_model("my_model")
    return model

import numpy as np
noisyPatches_train_1 = np.load('/content/gdrive/MyDrive/Colab Notebooks/EE610_FINAL_PROJECT/Outputs/noisyPatches_train_bilinear_1_4.npy')
centralPixel_train_1 = np.load('/content/gdrive/MyDrive/Colab Notebooks/EE610_FINAL_PROJECT/Outputs/centralPixel_train_bilinear_1_4.npy')
noisyPatches_val_1 = np.load('/content/gdrive/MyDrive/Colab Notebooks/EE610_FINAL_PROJECT/Outputs/noisyPatches_val_bilinear_1_4.npy')
centralPixel_val_1 = np.load('/content/gdrive/MyDrive/Colab Notebooks/EE610_FINAL_PROJECT/Outputs/centralPixel_val_bilinear_1_4.npy')
noisyPatches_test_1 = np.load('/content/gdrive/MyDrive/Colab Notebooks/EE610_FINAL_PROJECT/Outputs/noisyPatches_test_bilinear_1_4.npy')
centralPixel_test_1 = np.load('/content/gdrive/MyDrive/Colab Notebooks/EE610_FINAL_PROJECT/Outputs/centralPixel_test_bilinear_1_4.npy')

noisyPatches_train_sub = subPatchVectors(noisyPatches_train_1,7)
noisyPatches_val_sub = subPatchVectors(noisyPatches_val_1,7)
noisyPatches_test_sub = subPatchVectors(noisyPatches_test_1,7)

model2_7_1_4_bilinear = createModel_2(noisyPatches_train_sub,centralPixel_train_1,noisyPatches_val_sub,centralPixel_val_1,noisyPatches_test_sub,centralPixel_test_1,7,"/content/gdrive/MyDrive/Colab Notebooks/EE610_FINAL_PROJECT/Outputs/model2_7_1_4_bilinear.h5")

corrrected_image1 =reconstructImage_CNN('/content/gdrive/MyDrive/Colab Notebooks/EE610_FINAL_PROJECT/Inputs/butterfly_low_quality_bilinear.png',model2_7_1_4_bilinear,7)
cv2.imwrite('/content/gdrive/MyDrive/Colab Notebooks/EE610_FINAL_PROJECT/Outputs/butterfly_low_quality_bilinear_model2_7_1_4_bilinear.png', corrrected_image1)

noise_downsampled,  noise_reconstructed=calculateNoise('/content/gdrive/MyDrive/Colab Notebooks/EE610_FINAL_PROJECT/Inputs/butterfly.jpg','/content/gdrive/MyDrive/Colab Notebooks/EE610_FINAL_PROJECT/Inputs/butterfly_low_quality_bilinear.png','/content/gdrive/MyDrive/Colab Notebooks/EE610_FINAL_PROJECT/Outputs/butterfly_low_quality_bilinear_model2_7_1_4_bilinear.png')
print("Noise in Downsampled and Upsampled Image :",noise_downsampled)
print("Noise in reconstructed Image :",noise_reconstructed)

corrrected_image1 =reconstructImage_CNN('/content/gdrive/MyDrive/Colab Notebooks/EE610_FINAL_PROJECT/Inputs/228_low_quality_bilinear.png',model2_7_1_4_bilinear,7)
cv2.imwrite('/content/gdrive/MyDrive/Colab Notebooks/EE610_FINAL_PROJECT/Outputs/228_low_quality_bilinear_model2_7_1_4_bilinear.png', corrrected_image1)
noise_downsampled,  noise_reconstructed=calculateNoise('/content/gdrive/MyDrive/Colab Notebooks/EE610_FINAL_PROJECT/Inputs/228.png','/content/gdrive/MyDrive/Colab Notebooks/EE610_FINAL_PROJECT/Inputs/228_low_quality_bilinear.png','/content/gdrive/MyDrive/Colab Notebooks/EE610_FINAL_PROJECT/Outputs/228_low_quality_bilinear_model2_7_1_4_bilinear.png')
print("Noise in Downsampled and Upsampled Image :",noise_downsampled)
print("Noise in reconstructed Image :",noise_reconstructed)

# import numpy as np
import keras.models as models
import keras.layers as layers
from keras.models import Sequential
from keras.layers import Dense
import pandas as pd
from keras.callbacks import EarlyStopping
import numpy as np
from matplotlib import pyplot as plt

def createModel_5(noisyPatches_train1,centralPixel_train1,noisyPatches_val1,centralPixel_val1,noisyPatches_test1,centralPixel_test1,PatchSize,model_name):
    print("Traiing and Validation sets size :",noisyPatches_train1.shape,centralPixel_train1.shape,noisyPatches_val1.shape,centralPixel_val1.shape)
    
    if(PatchSize!=7):
      print("Expected Patch size is 7: Error condition")
      return

    row =noisyPatches_train1.shape[0] #507
    coloumn =noisyPatches_train1.shape[1]# 16250
    noisyPatches_train_1 = np.swapaxes(noisyPatches_train1,0,1)
    noisyPatches_train_1 = noisyPatches_train_1.reshape((coloumn,PatchSize,PatchSize,3))

    row =noisyPatches_val1.shape[0] #507
    coloumn =noisyPatches_val1.shape[1]# 16250
    noisyPatches_val_1 = np.swapaxes(noisyPatches_val1,0,1)
    noisyPatches_val_1 = noisyPatches_val_1.reshape((coloumn,PatchSize,PatchSize,3))

    row =noisyPatches_test1.shape[0] #507
    coloumn =noisyPatches_test1.shape[1]# 16250
    noisyPatches_test_1 = np.swapaxes(noisyPatches_test1,0,1)
    noisyPatches_test_1 = noisyPatches_test_1.reshape((coloumn,PatchSize,PatchSize,3))


    es = EarlyStopping(monitor='val_loss', mode='min',min_delta=0.0001, verbose=1,patience=15,restore_best_weights=True)
    model = models.Sequential()
    model.add(layers.Conv2D(256, (7, 7), activation='relu',padding='same', input_shape=(7, 7, 3)))
    model.add(layers.MaxPooling2D((2, 2)))
    model.add(layers.Conv2D(256, (5, 5), activation='relu',padding='same'))
    model.add(layers.MaxPooling2D((2, 2)))
    model.add(layers.Conv2D(256, (3, 3), activation='relu',padding='same'))
    model.add(layers.Flatten())
    model.add(layers.Dense(64, activation='relu'))
    model.add(layers.Dense(10))
    model.add(layers.Dense(3, activation='sigmoid'))
    model.summary()
    model.compile(loss='mse', optimizer='adam', metrics=['accuracy'])
    history = model.fit(noisyPatches_train_1, centralPixel_train1.T, epochs=1000,callbacks=[es],validation_data=(noisyPatches_val_1, centralPixel_val1.T))

    pd.DataFrame(history.history).plot(figsize=(8, 5))
    plt.grid(True)
    plt.gca().set_ylim(0, 1) # set the vertical range to [0-1]
    plt.show()

    _, accuracy =model.evaluate(noisyPatches_test_1, centralPixel_test1.T)
    print('Accuracy: %.2f' % (accuracy*100))
    model.save(model_name) # Saving model information for later usage
    #reconstructed_model = keras.models.load_model("my_model")
    return model

import numpy as np
noisyPatches_train_1 = np.load('/content/gdrive/MyDrive/Colab Notebooks/EE610_FINAL_PROJECT/Outputs/noisyPatches_train_bilinear_1_4.npy')
centralPixel_train_1 = np.load('/content/gdrive/MyDrive/Colab Notebooks/EE610_FINAL_PROJECT/Outputs/centralPixel_train_bilinear_1_4.npy')
noisyPatches_val_1 = np.load('/content/gdrive/MyDrive/Colab Notebooks/EE610_FINAL_PROJECT/Outputs/noisyPatches_val_bilinear_1_4.npy')
centralPixel_val_1 = np.load('/content/gdrive/MyDrive/Colab Notebooks/EE610_FINAL_PROJECT/Outputs/centralPixel_val_bilinear_1_4.npy')
noisyPatches_test_1 = np.load('/content/gdrive/MyDrive/Colab Notebooks/EE610_FINAL_PROJECT/Outputs/noisyPatches_test_bilinear_1_4.npy')
centralPixel_test_1 = np.load('/content/gdrive/MyDrive/Colab Notebooks/EE610_FINAL_PROJECT/Outputs/centralPixel_test_bilinear_1_4.npy')

noisyPatches_train_sub = subPatchVectors(noisyPatches_train_1,7)
noisyPatches_val_sub = subPatchVectors(noisyPatches_val_1,7)
noisyPatches_test_sub = subPatchVectors(noisyPatches_test_1,7)

model5_7_1_4_bilinear = createModel_5(noisyPatches_train_sub,centralPixel_train_1,noisyPatches_val_sub,centralPixel_val_1,noisyPatches_test_sub,centralPixel_test_1,7,"/content/gdrive/MyDrive/Colab Notebooks/EE610_FINAL_PROJECT/Outputs/model5_7_1_4_bilinear.h5")

corrrected_image1 =reconstructImage_CNN('/content/gdrive/MyDrive/Colab Notebooks/EE610_FINAL_PROJECT/Inputs/228_low_quality_bilinear.png',model5_7_1_4_bilinear,7)
cv2.imwrite('/content/gdrive/MyDrive/Colab Notebooks/EE610_FINAL_PROJECT/Outputs/228_low_quality_bilinear_model5_7_1_4_bilinear.png', corrrected_image1)

noise_downsampled,  noise_reconstructed=calculateNoise('/content/gdrive/MyDrive/Colab Notebooks/EE610_FINAL_PROJECT/Inputs/228.png','/content/gdrive/MyDrive/Colab Notebooks/EE610_FINAL_PROJECT/Inputs/228_low_quality_bilinear.png','/content/gdrive/MyDrive/Colab Notebooks/EE610_FINAL_PROJECT/Outputs/228_low_quality_bilinear_model7_7_1_4_bilinear.png')
print("Noise in Downsampled and Upsampled Image :",noise_downsampled)
print("Noise in reconstructed Image :",noise_reconstructed)

def extract_patches(full_imgs, crop_size, stride_size):
    patch_height = crop_size
    patch_width = crop_size
    stride_height = stride_size
    stride_width = stride_size

    assert (len(full_imgs.shape) == 4)  # 4D arrays
    img_h = full_imgs.shape[1]  # height of the full image
    img_w = full_imgs.shape[2]  # width of the full image

    assert ((img_h - patch_height) % stride_height == 0 and (img_w - patch_width) % stride_width == 0)
    N_patches_img = ((img_h - patch_height) // stride_height + 1) * (
            (img_w - patch_width) // stride_width + 1)  # // --> division between integers
    N_patches_tot = N_patches_img * full_imgs.shape[0]

    patches = np.empty((N_patches_tot, patch_height, patch_width, full_imgs.shape[3]))
    iter_tot = 0  # iter over the total number of patches (N_patches)
    for i in range(full_imgs.shape[0]):  # loop over the full images
        for h in range((img_h - patch_height) // stride_height + 1):
            for w in range((img_w - patch_width) // stride_width + 1):
                patch = full_imgs[i, h * stride_height:(h * stride_height) + patch_height,
                        w * stride_width:(w * stride_width) + patch_width, :]
                patches[iter_tot] = patch
                iter_tot += 1  # total
    assert (iter_tot == N_patches_tot)
    return patches

# import numpy as np
import keras.models as models
import keras.layers as layers
from keras.models import Sequential
from keras.layers import Dense
import pandas as pd
from keras.callbacks import EarlyStopping
import numpy as np
from matplotlib import pyplot as plt

def createModel_SRCNN(noisyPatches_train1,centralPixel_train1,noisyPatches_val1,centralPixel_val1,noisyPatches_test1,centralPixel_test1,PatchSize,model_name):
    print("Traiing and Validation sets size :",noisyPatches_train1.shape,centralPixel_train1.shape,noisyPatches_val1.shape,centralPixel_val1.shape)
    
    if(PatchSize!=7):
      print("Expected Patch size is 7: Error condition")
      return
    row =noisyPatches_train1.shape[0] #507
    coloumn =noisyPatches_train1.shape[1]# 16250
    noisyPatches_train_1 = np.swapaxes(noisyPatches_train1,0,1)
    noisyPatches_train_1 = noisyPatches_train_1.reshape((coloumn,PatchSize,PatchSize,3))

    row =noisyPatches_val1.shape[0] #507
    coloumn =noisyPatches_val1.shape[1]# 16250
    noisyPatches_val_1 = np.swapaxes(noisyPatches_val1,0,1)
    noisyPatches_val_1 = noisyPatches_val_1.reshape((coloumn,PatchSize,PatchSize,3))

    row =centralPixel_train1.shape[0] #507
    coloumn =centralPixel_train1.shape[1]# 16250
    centralPixel_train_1 = np.swapaxes(centralPixel_train1,0,1)
    centralPixel_train_1 = centralPixel_train_1.reshape((coloumn,PatchSize,PatchSize,3))

    row =centralPixel_val1.shape[0] #507
    coloumn =centralPixel_val1.shape[1]# 16250
    centralPixel_val_1 = np.swapaxes(centralPixel_val1,0,1)
    centralPixel_val_1 = centralPixel_val_1.reshape((coloumn,PatchSize,PatchSize,3))

    
    es = EarlyStopping(monitor='val_loss', mode='min',min_delta=0.0001, verbose=1,patience=15,restore_best_weights=True)
    model = Sequential()
    model.add(layers.Conv2D(64,9,padding='same',input_shape=(7,7,3),activation='relu'))
    model.add(layers.Conv2D(32,1,padding='same',activation='relu'))
    model.add(layers.Conv2D(3,5,padding='same'))
    model.summary()
    model.compile(loss='mse', optimizer='adam', metrics=['accuracy'])
    print("Traiing and Validation sets size :",noisyPatches_train1.shape,centralPixel_train1.shape,noisyPatches_val1.shape,centralPixel_val1.shape)
    history = model.fit(noisyPatches_train_1, centralPixel_train_1, epochs=1000,callbacks=[es],validation_data=(noisyPatches_val_1, centralPixel_val_1))

    pd.DataFrame(history.history).plot(figsize=(8, 5))
    plt.grid(True)
    plt.gca().set_ylim(0, 1) # set the vertical range to [0-1]
    plt.show()

   # _, accuracy =model.evaluate(noisyPatches_test_1, centralPixel_test1.T)
   # print('Accuracy: %.2f' % (accuracy*100))
    return model

import numpy as np
noisyPatches_train_1 = np.load('/content/gdrive/MyDrive/Colab Notebooks/EE610_FINAL_PROJECT/Outputs/noisyPatches_train_bilinear_1_4_SRCNN.npy')
centralPixel_train_1 = np.load('/content/gdrive/MyDrive/Colab Notebooks/EE610_FINAL_PROJECT/Outputs/centralPixel_train_bilinear_1_4_SRCNN.npy')
noisyPatches_val_1 = np.load('/content/gdrive/MyDrive/Colab Notebooks/EE610_FINAL_PROJECT/Outputs/noisyPatches_val_bilinear_1_4_SRCNN.npy')
centralPixel_val_1 = np.load('/content/gdrive/MyDrive/Colab Notebooks/EE610_FINAL_PROJECT/Outputs/centralPixel_val_bilinear_1_4_SRCNN.npy')
noisyPatches_test_1 = np.load('/content/gdrive/MyDrive/Colab Notebooks/EE610_FINAL_PROJECT/Outputs/noisyPatches_test_bilinear_1_4_SRCNN.npy')
centralPixel_test_1 = np.load('/content/gdrive/MyDrive/Colab Notebooks/EE610_FINAL_PROJECT/Outputs/centralPixel_test_bilinear_1_4_SRCNN.npy')

noisyPatches_train_sub = subPatchVectors(noisyPatches_train_1,7)
noisyPatches_val_sub = subPatchVectors(noisyPatches_val_1,7)
noisyPatches_test_sub = subPatchVectors(noisyPatches_test_1,7)

CentralPatches_train_sub = subPatchVectors(centralPixel_train_1,7)
CentralPatches_val_sub = subPatchVectors(centralPixel_val_1,7)
CentralPatches_test_sub = subPatchVectors(centralPixel_test_1,7)

print(noisyPatches_train_sub.shape,CentralPatches_train_sub.shape)

model_SRCNN = createModel_SRCNN(noisyPatches_train_sub,CentralPatches_train_sub,noisyPatches_val_sub,CentralPatches_val_sub,noisyPatches_test_sub,CentralPatches_test_sub,7,"/content/gdrive/MyDrive/Colab Notebooks/EE610_FINAL_PROJECT/Outputs/model_SRCNN.h5")

global W
import cv2

global model
from sklearn.datasets import load_sample_image
from sklearn.feature_extraction import image

def reconstructImage_SRCNN(filepath,model_,sub_patch):
    img = cv2.imread(filepath)
    #print(img.shape)
    #rgb_img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)

    img = img.astype(np.float64)
    img = img/255.0

    border = int(sub_patch/2)
    output_image = np.zeros((img.shape[0],img.shape[1],img.shape[2]),dtype=np.int8)
    #temp_image = cv2.copyMakeBorder(img, border, border, border, border, cv2.BORDER_REFLECT)
    for r in range(0,img.shape[0] - sub_patch, sub_patch):
        for c in range(0,img.shape[1] - sub_patch, sub_patch):
            window = img[r:r+sub_patch,c:c+sub_patch,:]
            window = window.reshape(1,7,7,3)
            predicted_value =model_.predict(window)
            predicted_value = predicted_value*254.0;
            predicted_value = predicted_value.astype(np.uint8)
            output_image[r:r+sub_patch,c:c+sub_patch] = predicted_value
    #rgb_img = cv2.cvtColor(output_image, cv2.COLOR_RGB2BGR)
    return output_image

corrrected_image1 =reconstructImage_SRCNN('/content/gdrive/MyDrive/Colab Notebooks/EE610_FINAL_PROJECT/Inputs/228_low_quality_bilinear.png',model_SRCNN,7)
cv2.imwrite('/content/gdrive/MyDrive/Colab Notebooks/EE610_FINAL_PROJECT/Outputs/228_low_quality_bilinear_model_SRCNN_.png', corrrected_image1)

noise_downsampled,  noise_reconstructed=calculateNoise('/content/gdrive/MyDrive/Colab Notebooks/EE610_FINAL_PROJECT/Inputs/228.png','/content/gdrive/MyDrive/Colab Notebooks/EE610_FINAL_PROJECT/Inputs/228_low_quality_bilinear.png','/content/gdrive/MyDrive/Colab Notebooks/EE610_FINAL_PROJECT/Outputs/228_low_quality_bilinear_model_SRCNN_.png')
print("Noise in Downsampled and Upsampled Image :",noise_downsampled)
print("Noise in reconstructed Image :",noise_reconstructed)

import matplotlib.image as mpimg
def rgbtoGrayScale(imagepath,savepath):
  img = cv2.imread(imagepath)
  temp =imagepath.split("/")
  file_output_path=savepath+"/"+temp[len(temp)-1].split(".")[0]+"gray.png"
  lum_img = img[:,:,0]
  cv2.imwrite(file_output_path,lum_img)

rgbtoGrayScale('/content/gdrive/MyDrive/Colab Notebooks/EE610_FINAL_PROJECT/Inputs/228_low_quality_bilinear.png','/content/gdrive/MyDrive/Colab Notebooks/EE610_FINAL_PROJECT/Inputs')

original_path = '/content/gdrive/MyDrive/Colab Notebooks/EE610_FINAL_PROJECT/DIV2KDATASET_US_1_4_BILINEAR/Test'
imageNames_original = []
os.scandir(original_path)

for i in os.scandir(original_path):
  if(str(i.path).find("Thumbs") ==-1): # To eliminate Thumbs.db from list
      imageNames_original.append(i.path)

for i in range(len(imageNames_original)):
  rgbtoGrayScale(imageNames_original[i],'/content/gdrive/MyDrive/Colab Notebooks/EE610_FINAL_PROJECT/DIV2KDATASET_US_1_4_BILINEAR_GRAY/Test')

original_path = '/content/gdrive/MyDrive/Colab Notebooks/EE610_FINAL_PROJECT/DIV2KDATASET_US_1_4_BILINEAR/Train'
imageNames_original = []
os.scandir(original_path)

for i in os.scandir(original_path):
  if(str(i.path).find("Thumbs") ==-1): # To eliminate Thumbs.db from list
      imageNames_original.append(i.path)

for i in range(len(imageNames_original)):
  rgbtoGrayScale(imageNames_original[i],'/content/gdrive/MyDrive/Colab Notebooks/EE610_FINAL_PROJECT/DIV2KDATASET_US_1_4_BILINEAR_GRAY/Train')

original_path = '/content/gdrive/MyDrive/Colab Notebooks/EE610_FINAL_PROJECT/DIV2KDATASET_US_1_4_BILINEAR/Validation'
imageNames_original = []
os.scandir(original_path)

for i in os.scandir(original_path):
  if(str(i.path).find("Thumbs") ==-1): # To eliminate Thumbs.db from list
      imageNames_original.append(i.path)

for i in range(len(imageNames_original)):
  rgbtoGrayScale(imageNames_original[i],'/content/gdrive/MyDrive/Colab Notebooks/EE610_FINAL_PROJECT/DIV2KDATASET_US_1_4_BILINEAR_GRAY/Validation')

original_path = '/content/gdrive/MyDrive/Colab Notebooks/EE610_FINAL_PROJECT/DIV2KDATASET_HR/Test'
imageNames_original = []
os.scandir(original_path)

for i in os.scandir(original_path):
  if(str(i.path).find("Thumbs") ==-1): # To eliminate Thumbs.db from list
      imageNames_original.append(i.path)

for i in range(len(imageNames_original)):
  rgbtoGrayScale(imageNames_original[i],'/content/gdrive/MyDrive/Colab Notebooks/EE610_FINAL_PROJECT/DIV2KDATASET_HR_Gray/Test')

original_path = '/content/gdrive/MyDrive/Colab Notebooks/EE610_FINAL_PROJECT/DIV2KDATASET_HR/Train'
imageNames_original = []
os.scandir(original_path)

for i in os.scandir(original_path):
  if(str(i.path).find("Thumbs") ==-1): # To eliminate Thumbs.db from list
      imageNames_original.append(i.path)

for i in range(len(imageNames_original)):
  rgbtoGrayScale(imageNames_original[i],'/content/gdrive/MyDrive/Colab Notebooks/EE610_FINAL_PROJECT/DIV2KDATASET_HR_Gray/Train')

original_path = '/content/gdrive/MyDrive/Colab Notebooks/EE610_FINAL_PROJECT/DIV2KDATASET_HR/Validation'
imageNames_original = []
os.scandir(original_path)

for i in os.scandir(original_path):
  if(str(i.path).find("Thumbs") ==-1): # To eliminate Thumbs.db from list
      imageNames_original.append(i.path)

for i in range(len(imageNames_original)):
  rgbtoGrayScale(imageNames_original[i],'/content/gdrive/MyDrive/Colab Notebooks/EE610_FINAL_PROJECT/DIV2KDATASET_HR_Gray/Validation')

original_path = '/content/gdrive/MyDrive/Colab Notebooks/EE610_FINAL_PROJECT/DIV2KDATASET_HR/Test'
imageNames_original = []
os.scandir(original_path)

for i in os.scandir(original_path):
  if(str(i.path).find("Thumbs") ==-1): # To eliminate Thumbs.db from list
      imageNames_original.append(i.path)

for i in range(len(imageNames_original)):
  rgbtoGrayScale(imageNames_original[i],'/content/gdrive/MyDrive/Colab Notebooks/EE610_FINAL_PROJECT/DIV2KDATASET_HR/Test_Gray')

import random
import numpy as np
import matplotlib.pyplot as plt

import pywt
import pywt.data
W=500
img = cv2.imread('/content/gdrive/MyDrive/Colab Notebooks/EE610_FINAL_PROJECT/DIV2KDATASET_HR/Test_Gray/0228gray.png')
x = random.randint(0, (img.shape[0]//W)-1) # Random x location so that the patch,remains within image boundaries
y = random.randint(0, (img.shape[1]//W)-1) # Random y location,remains within image boundaries
temp_data = np.copy(img[(x*W):((x*W)+W),(y*W):((y*W)+W),:])
cv2.imwrite('/content/gdrive/MyDrive/Colab Notebooks/EE610_FINAL_PROJECT/Outputs/tset.png',temp_data)




# Load image
original = temp_data[:,:,0]

# Wavelet transform of image, and plot approximation and details
titles = ['Approximation', ' Horizontal detail',
          'Vertical detail', 'Diagonal detail']
print(original.shape)
coeffs2 = pywt.dwt2(original, 'haar')
LL, (LH, HL, HH) = coeffs2
print(LL.shape)

reconstructed = pywt.idwt2(coeffs2, 'bior1.3')
print(reconstructed.shape)
cv2.imwrite('/content/gdrive/MyDrive/Colab Notebooks/EE610_FINAL_PROJECT/Outputs/tset2.png',reconstructed)

import random
import numpy as np
import matplotlib.pyplot as plt

import pywt
import pywt.data


def createPatches_Gray(filepath,filepath_original,test_or_validation,patch_size): 
  global noisyPatches_train_LL
  global centralPixel_train_LL
  global noisyPatches_val_LL
  global centralPixel_val_LL

  global noisyPatches_train_LH
  global centralPixel_train_LH
  global noisyPatches_val_LH
  global centralPixel_val_LH

  global noisyPatches_train_HL
  global centralPixel_train_HL
  global noisyPatches_val_HL
  global centralPixel_val_HL

  global noisyPatches_train_HH
  global centralPixel_train_HH
  global noisyPatches_val_HH
  global centralPixel_val_HH

  img = cv2.imread(filepath)  
  img_original = cv2.imread(filepath_original)  
  if((img.shape[0]<300) or (img.shape[1]<300)):
      return   
  W=101
  numPatches =patch_size # Extracting patches from image and normalizing
  for i in range(numPatches): # For number of patches

      x = random.randint(0, (img.shape[0]//W)-1) # Random x location so that the patch,remains within image boundaries
      y = random.randint(0, (img.shape[1]//W)-1) # Random y location,remains within image boundaries
      temp_data = np.copy(img[(x*W):((x*W)+W),(y*W):((y*W)+W),:])
      temp_data_central = np.copy(img_original[(x*W):((x*W)+W),(y*W):((y*W)+W),:])

      temp_data_ = temp_data[:,:,0]
      temp_data_central_ =temp_data_central[:,:,0]
      
      coeffs2_data = pywt.dwt2(temp_data_, 'db1')
      LL_data, (LH_data, HL_data, HH_data) = coeffs2_data

      coeffs2_central = pywt.dwt2(temp_data_central_, 'db1')
      LL_central, (LH_central, HL_central, HH_central) = coeffs2_central

      Y = LL_central.shape[0]

      #LL_data = LL_data/255.0
      #LH_data = LH_data/255.0
      #HL_data =HL_data/255.0
      #HH_data = HH_data/255.0

      #LL_central = LL_central/255.0
      #LH_central = LH_central/255.0
      #HL_central = HL_central/255.0
      #HH_central = HH_central/255.0

      LL_data =LL_central-LL_data
      LH_data =LH_central-LH_data
      HL_data =HL_central-HL_data
      HH_data =HH_central-HH_data


      LL_data = LL_data.reshape((Y*Y),1)
      LH_data = LH_data.reshape((Y*Y),1)
      HL_data = HL_data.reshape((Y*Y),1)
      HH_data = HH_data.reshape((Y*Y),1)
      LL_central = LL_central.reshape((Y*Y),1)
      LH_central = LH_central.reshape((Y*Y),1)
      HL_central = HL_central.reshape((Y*Y),1)
      HH_central = HH_central.reshape((Y*Y),1)
      
      if(test_or_validation =="TRAIN"): #Appending the noise patches to numpy array for later usage
          noisyPatches_train_LL= np.append(noisyPatches_train_LL,LL_data,axis=1)
          centralPixel_train_LL=np.append(centralPixel_train_LL,LL_central,axis=1)
          noisyPatches_train_LH= np.append(noisyPatches_train_LH,LH_data,axis=1)
          centralPixel_train_LH=np.append(centralPixel_train_LH,LH_central,axis=1)
          noisyPatches_train_HL= np.append(noisyPatches_train_HL,HL_data,axis=1)
          centralPixel_train_HL=np.append(centralPixel_train_HL,HL_central,axis=1)
          noisyPatches_train_HH= np.append(noisyPatches_train_HH,HH_data,axis=1)
          centralPixel_train_HH=np.append(centralPixel_train_HH,HH_central,axis=1)
      elif(test_or_validation =="VALIDATE"):
          noisyPatches_val_LL= np.append(noisyPatches_val_LL,LL_data,axis=1)
          centralPixel_val_LL=np.append(centralPixel_val_LL,LL_central,axis=1)
          noisyPatches_val_LH= np.append(noisyPatches_val_LH,LH_data,axis=1)
          centralPixel_val_LH=np.append(centralPixel_val_LH,LH_central,axis=1)
          noisyPatches_val_HL= np.append(noisyPatches_val_HL,HL_data,axis=1)
          centralPixel_val_HL=np.append(centralPixel_val_HL,HL_central,axis=1)
          noisyPatches_val_HH= np.append(noisyPatches_val_HH,HH_data,axis=1)
          centralPixel_val_HH=np.append(centralPixel_val_HH,HH_central,axis=1)
      elif(test_or_validation =="TEST"):
          noisyPatches_test_LL= np.append(noisyPatches_test_LL,LL_data,axis=1)
          centralPixel_test_LL=np.append(centralPixel_test_LL,LL_central,axis=1)
          noisyPatches_test_LH= np.append(noisyPatches_test_LH,LH_data,axis=1)
          centralPixel_test_LH=np.append(centralPixel_test_LH,LH_central,axis=1)
          noisyPatches_test_HL= np.append(noisyPatches_test_HL,HL_data,axis=1)
          centralPixel_test_HL=np.append(centralPixel_test_HL,HL_central,axis=1)
          noisyPatches_test_HH= np.append(noisyPatches_test_HH,HH_data,axis=1)
          centralPixel_test_HH=np.append(centralPixel_test_HH,HH_central,axis=1)

global noisyPatches_train_LL
global centralPixel_train_LL
global noisyPatches_val_LL
global centralPixel_val_LL

global noisyPatches_train_LH
global centralPixel_train_LH
global noisyPatches_val_LH
global centralPixel_val_LH

global noisyPatches_train_HL
global centralPixel_train_HL
global noisyPatches_val_HL
global centralPixel_val_HL

global noisyPatches_train_HH
global centralPixel_train_HH
global noisyPatches_val_HH
global centralPixel_val_HH    
W=51
noisyPatches_train_LL = np.zeros((W*W,0),dtype=np.float64)
centralPixel_train_LL = np.zeros((W*W,0),dtype=np.float64)
noisyPatches_val_LL = np.zeros((W*W,0),dtype=np.float64)
centralPixel_val_LL = np.zeros((W*W,0),dtype=np.float64)
noisyPatches_train_LH = np.zeros((W*W,0),dtype=np.float64)
centralPixel_train_LH = np.zeros((W*W,0),dtype=np.float64)
noisyPatches_val_LH = np.zeros((W*W,0),dtype=np.float64)
centralPixel_val_LH = np.zeros((W*W,0),dtype=np.float64)
noisyPatches_train_HL = np.zeros((W*W,0),dtype=np.float64)
centralPixel_train_HL = np.zeros((W*W,0),dtype=np.float64)
noisyPatches_val_HL = np.zeros((W*W,0),dtype=np.float64)
centralPixel_val_HL = np.zeros((W*W,0),dtype=np.float64)
noisyPatches_train_HH = np.zeros((W*W,0),dtype=np.float64)
centralPixel_train_HH = np.zeros((W*W,0),dtype=np.float64)
noisyPatches_val_HH = np.zeros((W*W,0),dtype=np.float64)
centralPixel_val_HH = np.zeros((W*W,0),dtype=np.float64)


train_path = '/content/gdrive/MyDrive/Colab Notebooks/EE610_FINAL_PROJECT/DIV2KDATASET_US_1_4_BILINEAR_GRAY/Train'
val_path = '/content/gdrive/MyDrive/Colab Notebooks/EE610_FINAL_PROJECT/DIV2KDATASET_US_1_4_BILINEAR_GRAY/Validation'
test_path = '/content/gdrive/MyDrive/Colab Notebooks/EE610_FINAL_PROJECT/DIV2KDATASET_US_1_4_BILINEAR_GRAY/Test'
original_train_path = '/content/gdrive/MyDrive/Colab Notebooks/EE610_FINAL_PROJECT/DIV2KDATASET_HR_Gray/Train'
original_test_path = '/content/gdrive/MyDrive/Colab Notebooks/EE610_FINAL_PROJECT/DIV2KDATASET_HR_Gray/Test'
original_val_path = '/content/gdrive/MyDrive/Colab Notebooks/EE610_FINAL_PROJECT/DIV2KDATASET_HR_Gray/Validation'

imageNames_train = []
imageNames_val = []
imageNames_test = []
original_imageNames_train =[]
original_imageNames_test =[]
original_imageNames_val =[]

os.scandir(train_path)

for i in os.scandir(train_path):
  if(str(i.path).find("Thumbs") ==-1): # To eliminate Thumbs.db from list
      imageNames_train.append(i.path)
for i in os.scandir(val_path):
  if(str(i.path).find("Thumbs") ==-1): # To eliminate Thumbs.db from list
      imageNames_val.append(i.path)
for i in os.scandir(test_path):
  if(str(i.path).find("Thumbs") ==-1): # To eliminate Thumbs.db from list
      imageNames_test.append(i.path)
for i in os.scandir(original_train_path):
  if(str(i.path).find("Thumbs") ==-1): # To eliminate Thumbs.db from list
      original_imageNames_train.append(i.path)
for i in os.scandir(original_test_path):
  if(str(i.path).find("Thumbs") ==-1): # To eliminate Thumbs.db from list
      original_imageNames_test.append(i.path)
for i in os.scandir(original_val_path):
  if(str(i.path).find("Thumbs") ==-1): # To eliminate Thumbs.db from list
      original_imageNames_val.append(i.path)
        
imageNames_train =sorted(imageNames_train)     
original_imageNames_train =sorted(original_imageNames_train)  
imageNames_test =sorted(imageNames_test)     
original_imageNames_test =sorted(original_imageNames_test) 
imageNames_val =sorted(imageNames_val)     
original_imageNames_val =sorted(original_imageNames_val)   
        
training_set_size =len(imageNames_train)
print("The no of images in the Train directory is : "+str(training_set_size))

val_set_size =len(imageNames_val)
print("The no of images in the Validation directory is : "+str(val_set_size))

test_set_size =len(imageNames_test)
print("The no of images in the Test directory is : "+str(test_set_size))

from matplotlib import pyplot as plt
import cv2
import numpy as np
import random
global noisyPatches_train_LL
global centralPixel_train_LL
global noisyPatches_val_LL
global centralPixel_val_LL

global noisyPatches_train_LH
global centralPixel_train_LH
global noisyPatches_val_LH
global centralPixel_val_LH

global noisyPatches_train_HL
global centralPixel_train_HL
global noisyPatches_val_HL
global centralPixel_val_HL

global noisyPatches_train_HH
global centralPixel_train_HH
global noisyPatches_val_HH
global centralPixel_val_HH

W=51
noisyPatches_train_LL = np.zeros((W*W,0),dtype=np.float64)
centralPixel_train_LL = np.zeros((W*W,0),dtype=np.float64)
noisyPatches_val_LL = np.zeros((W*W,0),dtype=np.float64)
centralPixel_val_LL = np.zeros((W*W,0),dtype=np.float64)
noisyPatches_train_LH = np.zeros((W*W,0),dtype=np.float64)
centralPixel_train_LH = np.zeros((W*W,0),dtype=np.float64)
noisyPatches_val_LH = np.zeros((W*W,0),dtype=np.float64)
centralPixel_val_LH = np.zeros((W*W,0),dtype=np.float64)
noisyPatches_train_HL = np.zeros((W*W,0),dtype=np.float64)
centralPixel_train_HL = np.zeros((W*W,0),dtype=np.float64)
noisyPatches_val_HL = np.zeros((W*W,0),dtype=np.float64)
centralPixel_val_HL = np.zeros((W*W,0),dtype=np.float64)
noisyPatches_train_HH = np.zeros((W*W,0),dtype=np.float64)
centralPixel_train_HH = np.zeros((W*W,0),dtype=np.float64)
noisyPatches_val_HH = np.zeros((W*W,0),dtype=np.float64)
centralPixel_val_HH = np.zeros((W*W,0),dtype=np.float64)

for i in range(len(imageNames_train)):
    createPatches_Gray(imageNames_train[i],original_imageNames_train[i],"TRAIN",20)
for i in range(len(imageNames_val)):
    createPatches_Gray(imageNames_val[i],original_imageNames_val[i],"VALIDATE",20)



    
np.save('/content/gdrive/MyDrive/Colab Notebooks/EE610_FINAL_PROJECT/Outputs/noisyPatches_train_LL', noisyPatches_train_LL)
np.save('/content/gdrive/MyDrive/Colab Notebooks/EE610_FINAL_PROJECT/Outputs/centralPixel_train_LL', centralPixel_train_LL)
np.save('/content/gdrive/MyDrive/Colab Notebooks/EE610_FINAL_PROJECT/Outputs/noisyPatches_val_LL', noisyPatches_val_LL)
np.save('/content/gdrive/MyDrive/Colab Notebooks/EE610_FINAL_PROJECT/Outputs/centralPixel_val_LL', centralPixel_val_LL)

np.save('/content/gdrive/MyDrive/Colab Notebooks/EE610_FINAL_PROJECT/Outputs/noisyPatches_train_LH', noisyPatches_train_LH)
np.save('/content/gdrive/MyDrive/Colab Notebooks/EE610_FINAL_PROJECT/Outputs/centralPixel_train_LH', centralPixel_train_LH)
np.save('/content/gdrive/MyDrive/Colab Notebooks/EE610_FINAL_PROJECT/Outputs/noisyPatches_val_LH', noisyPatches_val_LH)
np.save('/content/gdrive/MyDrive/Colab Notebooks/EE610_FINAL_PROJECT/Outputs/centralPixel_val_LH', centralPixel_val_LH)

np.save('/content/gdrive/MyDrive/Colab Notebooks/EE610_FINAL_PROJECT/Outputs/noisyPatches_train_HL', noisyPatches_train_HL)
np.save('/content/gdrive/MyDrive/Colab Notebooks/EE610_FINAL_PROJECT/Outputs/centralPixel_train_HL', centralPixel_train_HL)
np.save('/content/gdrive/MyDrive/Colab Notebooks/EE610_FINAL_PROJECT/Outputs/noisyPatches_val_HL', noisyPatches_val_HL)
np.save('/content/gdrive/MyDrive/Colab Notebooks/EE610_FINAL_PROJECT/Outputs/centralPixel_val_HL', centralPixel_val_HL)

np.save('/content/gdrive/MyDrive/Colab Notebooks/EE610_FINAL_PROJECT/Outputs/noisyPatches_train_HH', noisyPatches_train_HH)
np.save('/content/gdrive/MyDrive/Colab Notebooks/EE610_FINAL_PROJECT/Outputs/centralPixel_train_HH', centralPixel_train_HH)
np.save('/content/gdrive/MyDrive/Colab Notebooks/EE610_FINAL_PROJECT/Outputs/noisyPatches_val_HH', noisyPatches_val_HH)
np.save('/content/gdrive/MyDrive/Colab Notebooks/EE610_FINAL_PROJECT/Outputs/centralPixel_val_HH', centralPixel_val_HH)



print("Noise Patch (Train) shape : ",noisyPatches_train_LL.shape)
print("Central Pixel (Train) shape : ",centralPixel_train_LL.shape)
print("Noise Patch (Validation) shape : ",noisyPatches_val_LL.shape)
print("Central Pixel (Validation)  shape : ",centralPixel_val_LL.shape)
print("Noise Patch (Train) shape : ",noisyPatches_train_LH.shape)
print("Central Pixel (Train) shape : ",centralPixel_train_LH.shape)
print("Noise Patch (Validation) shape : ",noisyPatches_val_LH.shape)
print("Central Pixel (Validation)  shape : ",centralPixel_val_LH.shape)
print("Noise Patch (Train) shape : ",noisyPatches_train_HL.shape)
print("Central Pixel (Train) shape : ",centralPixel_train_HL.shape)
print("Noise Patch (Validation) shape : ",noisyPatches_val_HL.shape)
print("Central Pixel (Validation)  shape : ",centralPixel_val_HL.shape)
print("Noise Patch (Train) shape : ",noisyPatches_train_HH.shape)
print("Central Pixel (Train) shape : ",centralPixel_train_HH.shape)
print("Noise Patch (Validation) shape : ",noisyPatches_val_HH.shape)
print("Central Pixel (Validation)  shape : ",centralPixel_val_HH.shape)

np.save('/content/gdrive/MyDrive/Colab Notebooks/EE610_FINAL_PROJECT/Outputs/noisyPatches_train_LL', noisyPatches_train_LL)
np.save('/content/gdrive/MyDrive/Colab Notebooks/EE610_FINAL_PROJECT/Outputs/centralPixel_train_LL', centralPixel_train_LL)
np.save('/content/gdrive/MyDrive/Colab Notebooks/EE610_FINAL_PROJECT/Outputs/noisyPatches_val_LL', noisyPatches_val_LL)
np.save('/content/gdrive/MyDrive/Colab Notebooks/EE610_FINAL_PROJECT/Outputs/centralPixel_val_LL', centralPixel_val_LL)

np.save('/content/gdrive/MyDrive/Colab Notebooks/EE610_FINAL_PROJECT/Outputs/noisyPatches_train_LH', noisyPatches_train_LH)
np.save('/content/gdrive/MyDrive/Colab Notebooks/EE610_FINAL_PROJECT/Outputs/centralPixel_train_LH', centralPixel_train_LH)
np.save('/content/gdrive/MyDrive/Colab Notebooks/EE610_FINAL_PROJECT/Outputs/noisyPatches_val_LH', noisyPatches_val_LH)
np.save('/content/gdrive/MyDrive/Colab Notebooks/EE610_FINAL_PROJECT/Outputs/centralPixel_val_LH', centralPixel_val_LH)

np.save('/content/gdrive/MyDrive/Colab Notebooks/EE610_FINAL_PROJECT/Outputs/noisyPatches_train_HL', noisyPatches_train_HL)
np.save('/content/gdrive/MyDrive/Colab Notebooks/EE610_FINAL_PROJECT/Outputs/centralPixel_train_HL', centralPixel_train_HL)
np.save('/content/gdrive/MyDrive/Colab Notebooks/EE610_FINAL_PROJECT/Outputs/noisyPatches_val_HL', noisyPatches_val_HL)
np.save('/content/gdrive/MyDrive/Colab Notebooks/EE610_FINAL_PROJECT/Outputs/centralPixel_val_HL', centralPixel_val_HL)

np.save('/content/gdrive/MyDrive/Colab Notebooks/EE610_FINAL_PROJECT/Outputs/noisyPatches_train_HH', noisyPatches_train_HH)
np.save('/content/gdrive/MyDrive/Colab Notebooks/EE610_FINAL_PROJECT/Outputs/centralPixel_train_HH', centralPixel_train_HH)
np.save('/content/gdrive/MyDrive/Colab Notebooks/EE610_FINAL_PROJECT/Outputs/noisyPatches_val_HH', noisyPatches_val_HH)
np.save('/content/gdrive/MyDrive/Colab Notebooks/EE610_FINAL_PROJECT/Outputs/centralPixel_val_HH', centralPixel_val_HH)



print("Noise Patch (Train) shape : ",noisyPatches_train_LL.shape)
print("Central Pixel (Train) shape : ",centralPixel_train_LL.shape)
print("Noise Patch (Validation) shape : ",noisyPatches_val_LL.shape)
print("Central Pixel (Validation)  shape : ",centralPixel_val_LL.shape)
print("Noise Patch (Train) shape : ",noisyPatches_train_LH.shape)
print("Central Pixel (Train) shape : ",centralPixel_train_LH.shape)
print("Noise Patch (Validation) shape : ",noisyPatches_val_LH.shape)
print("Central Pixel (Validation)  shape : ",centralPixel_val_LH.shape)
print("Noise Patch (Train) shape : ",noisyPatches_train_HL.shape)
print("Central Pixel (Train) shape : ",centralPixel_train_HL.shape)
print("Noise Patch (Validation) shape : ",noisyPatches_val_HL.shape)
print("Central Pixel (Validation)  shape : ",centralPixel_val_HL.shape)
print("Noise Patch (Train) shape : ",noisyPatches_train_HH.shape)
print("Central Pixel (Train) shape : ",centralPixel_train_HH.shape)
print("Noise Patch (Validation) shape : ",noisyPatches_val_HH.shape)
print("Central Pixel (Validation)  shape : ",centralPixel_val_HH.shape)

import numpy as np
noisyPatches_train_1 = np.load('/content/gdrive/MyDrive/Colab Notebooks/EE610_FINAL_PROJECT/Outputs/noisyPatches_train_LL.npy')
centralPixel_train_1 = np.load('/content/gdrive/MyDrive/Colab Notebooks/EE610_FINAL_PROJECT/Outputs/centralPixel_train_LL.npy')
noisyPatches_val_1 = np.load('/content/gdrive/MyDrive/Colab Notebooks/EE610_FINAL_PROJECT/Outputs/noisyPatches_val_LL.npy')
centralPixel_val_1 = np.load('/content/gdrive/MyDrive/Colab Notebooks/EE610_FINAL_PROJECT/Outputs/centralPixel_val_LL.npy')

print("Noise Patch (Train) shape : ",noisyPatches_train_LL.shape)
print("Central Pixel (Train) shape : ",centralPixel_train_LL.shape)
print("Noise Patch (Validation) shape : ",noisyPatches_val_LL.shape)
print("Central Pixel (Validation)  shape : ",centralPixel_val_LL.shape)


model_LL = createModel_3(noisyPatches_train_1,noisyPatches_train_1,noisyPatches_val_1,centralPixel_val_1,51,"/content/gdrive/MyDrive/Colab Notebooks/EE610_FINAL_PROJECT/Outputs/model_LL.h5")

# import numpy as np
import keras.models as models
import keras.layers as layers
from keras.models import Sequential
from keras.layers import Dense
import pandas as pd
from keras.callbacks import EarlyStopping
import numpy as np
from matplotlib import pyplot as plt



def createModel_3(noisyPatches_train1,centralPixel_train1,noisyPatches_val1,centralPixel_val1,PatchSize,model_name):
    print("Traiing and Validation sets size :",noisyPatches_train1.shape,centralPixel_train1.shape,noisyPatches_val1.shape,centralPixel_val1.shape)
    
    if(PatchSize!=51):
      print("Expected Patch size is 51: Error condition")
      return
    row =noisyPatches_train1.shape[0] #507
    coloumn =noisyPatches_train1.shape[1]# 16250
    noisyPatches_train_1 = np.swapaxes(noisyPatches_train1,0,1)
    noisyPatches_train_1 = noisyPatches_train_1.reshape((coloumn,PatchSize,PatchSize,1))

    row =noisyPatches_val1.shape[0] #507
    coloumn =noisyPatches_val1.shape[1]# 16250
    noisyPatches_val_1 = np.swapaxes(noisyPatches_val1,0,1)
    noisyPatches_val_1 = noisyPatches_val_1.reshape((coloumn,PatchSize,PatchSize,1))

    row =centralPixel_train1.shape[0] #507
    coloumn =centralPixel_train1.shape[1]# 16250
    centralPixel_train_1 = np.swapaxes(centralPixel_train1,0,1)
    centralPixel_train_1 = centralPixel_train_1.reshape((coloumn,PatchSize,PatchSize,1))

    row =centralPixel_val1.shape[0] #507
    coloumn =centralPixel_val1.shape[1]# 16250
    centralPixel_val_1 = np.swapaxes(centralPixel_val1,0,1)
    centralPixel_val_1 = centralPixel_val_1.reshape((coloumn,PatchSize,PatchSize,1))

    
    es = EarlyStopping(monitor='val_loss', mode='min',min_delta=0.001, verbose=1,patience=100,restore_best_weights=True)
    model = Sequential()
    model.add(layers.Conv2D(8,9,padding='same',input_shape=(51,51,1),activation='relu'))
    model.add(layers.Conv2D(16,7,padding='same',activation='relu'))
    model.add(layers.Conv2D(1,5,padding='same',activation='relu'))
    model.summary()
    model.compile(loss='mse', optimizer='adam', metrics=['accuracy'])
    print("Traiing and Validation sets size :",noisyPatches_train1.shape,centralPixel_train1.shape,noisyPatches_val1.shape,centralPixel_val1.shape)
    history = model.fit(noisyPatches_train_1, centralPixel_train_1, epochs=10000,callbacks=[es],validation_data=(noisyPatches_val_1, centralPixel_val_1))

    pd.DataFrame(history.history).plot(figsize=(8, 5))
    plt.grid(True)
    plt.gca().set_ylim(0, 1) # set the vertical range to [0-1]
    plt.show()

   # _, accuracy =model.evaluate(noisyPatches_test_1, centralPixel_test1.T)
   # print('Accuracy: %.2f' % (accuracy*100))
    return model

global noisyPatches_train_LL
global centralPixel_train_LL
global noisyPatches_val_LL
global centralPixel_val_LL

global noisyPatches_train_LH
global centralPixel_train_LH
global noisyPatches_val_LH
global centralPixel_val_LH

global noisyPatches_train_HL
global centralPixel_train_HL
global noisyPatches_val_HL
global centralPixel_val_HL

global noisyPatches_train_HH
global centralPixel_train_HH
global noisyPatches_val_HH
global centralPixel_val_HH
W=51
noisyPatches_train_LL = np.zeros((W*W,0),dtype=np.float64)
centralPixel_train_LL = np.zeros((1,0),dtype=np.float64)
noisyPatches_val_LL = np.zeros((W*W,0),dtype=np.float64)
centralPixel_val_LL = np.zeros((1,0),dtype=np.float64)
noisyPatches_train_LH = np.zeros((W*W,0),dtype=np.float64)
centralPixel_train_LH = np.zeros((1,0),dtype=np.float64)
noisyPatches_val_LH = np.zeros((W*W,0),dtype=np.float64)
centralPixel_val_LH = np.zeros((1,0),dtype=np.float64)
noisyPatches_train_HL = np.zeros((W*W,0),dtype=np.float64)
centralPixel_train_HL = np.zeros((1,0),dtype=np.float64)
noisyPatches_val_HL = np.zeros((W*W,0),dtype=np.float64)
centralPixel_val_HL = np.zeros((1,0),dtype=np.float64)
noisyPatches_train_HH = np.zeros((W*W,0),dtype=np.float64)
centralPixel_train_HH = np.zeros((1,0),dtype=np.float64)
noisyPatches_val_HH = np.zeros((W*W,0),dtype=np.float64)
centralPixel_val_HH = np.zeros((1,0),dtype=np.float64)